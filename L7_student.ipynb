{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Of62xpmjWPGa"
   },
   "source": [
    "# Build an End-to-End System\n",
    "\n",
    "This puts together the chain of prompts that you saw throughout the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "#### Load the API key and relevant Python libaries.\n",
    "In this course, we've provided some code that loads the OpenAI API key for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"esms-options\">{\"shimMode\": true}</script><style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n  const py_version = '3.6.0'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  const reloading = false;\n  const Bokeh = root.Bokeh;\n\n  // Set a timeout for this load but only if we are not already initializing\n  if (typeof (root._bokeh_timeout) === \"undefined\" || (force || !root._bokeh_is_initializing)) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      // Don't load bokeh if it is still initializing\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    } else if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      // There is nothing to load\n      run_callbacks();\n      return null;\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error(e) {\n      const src_el = e.srcElement\n      console.error(\"failed to load \" + (src_el.href || src_el.src));\n    }\n\n    const skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {}, 'shim': {}});\n      root._bokeh_is_loading = css_urls.length + 0;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    const existing_stylesheets = []\n    const links = document.getElementsByTagName('link')\n    for (let i = 0; i < links.length; i++) {\n      const link = links[i]\n      if (link.href != null) {\n        existing_stylesheets.push(link.href)\n      }\n    }\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const escaped = encodeURI(url)\n      if (existing_stylesheets.indexOf(escaped) !== -1) {\n        on_load()\n        continue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    var existing_scripts = []\n    const scripts = document.getElementsByTagName('script')\n    for (let i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n        existing_scripts.push(script.src)\n      }\n    }\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (let i = 0; i < js_modules.length; i++) {\n      const url = js_modules[i];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      const url = js_exports[name];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) >= 0 || root[name] != null) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.holoviz.org/panel/1.5.3/dist/bundled/reactiveesm/es-module-shims@^1.10.0/dist/es-module-shims.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.6.0.min.js\", \"https://cdn.holoviz.org/panel/1.5.3/dist/panel.min.js\"];\n  const js_modules = [];\n  const js_exports = {};\n  const css_urls = [];\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (let i = 0; i < inline_js.length; i++) {\n        try {\n          inline_js[i].call(root, root.Bokeh);\n        } catch(e) {\n          if (!reloading) {\n            throw e;\n          }\n        }\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n        var NewBokeh = root.Bokeh;\n        if (Bokeh.versions === undefined) {\n          Bokeh.versions = new Map();\n        }\n        if (NewBokeh.version !== Bokeh.version) {\n          Bokeh.versions.set(NewBokeh.version, NewBokeh)\n        }\n        root.Bokeh = Bokeh;\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      // If the timeout and bokeh was not successfully loaded we reset\n      // everything and try loading again\n      root._bokeh_timeout = Date.now() + 5000;\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      root._bokeh_is_loading = 0\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      const bokeh_loaded = root.Bokeh != null && (root.Bokeh.version === py_version || (root.Bokeh.versions !== undefined && root.Bokeh.versions.has(py_version)));\n      if (!reloading && !bokeh_loaded) {\n        if (root.Bokeh) {\n          root.Bokeh = undefined;\n        }\n        console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n        console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n        run_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='00ca0b8c-4b6b-4519-94a9-22e79bf1bbef'>\n",
       "  <div id=\"a4b41170-e9ac-419d-b9c8-ad4f7b9c266b\" data-root-id=\"00ca0b8c-4b6b-4519-94a9-22e79bf1bbef\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"d9f75519-04a3-4816-95e5-f5096d025a4a\":{\"version\":\"3.6.0\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"00ca0b8c-4b6b-4519-94a9-22e79bf1bbef\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"1444e28d-c108-46d7-9ca8-f0051b529da2\",\"attributes\":{\"plot_id\":\"00ca0b8c-4b6b-4519-94a9-22e79bf1bbef\",\"comm_id\":\"7364cdc8d77e4360a05c8d1c6e094e2e\",\"client_comm_id\":\"d87fc499cc754973948487455577e007\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"ReactiveESM1\"},{\"type\":\"model\",\"name\":\"JSComponent1\"},{\"type\":\"model\",\"name\":\"ReactComponent1\"},{\"type\":\"model\",\"name\":\"AnyWidgetComponent1\"},{\"type\":\"model\",\"name\":\"request_value1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"_synced\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_request_sync\",\"kind\":\"Any\",\"default\":0}]}]}};\n",
       "  var render_items = [{\"docid\":\"d9f75519-04a3-4816-95e5-f5096d025a4a\",\"roots\":{\"00ca0b8c-4b6b-4519-94a9-22e79bf1bbef\":\"a4b41170-e9ac-419d-b9c8-ad4f7b9c266b\"},\"root_ids\":[\"00ca0b8c-4b6b-4519-94a9-22e79bf1bbef\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  async function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    await Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && id_el.children[0].hasAttribute('data-root-id')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t  for (const child of root_el.children) {\n",
       "            // Ensure JupyterLab does not capture keyboard shortcuts\n",
       "            // see: https://jupyterlab.readthedocs.io/en/4.1.x/extension/notebook.html#keyboard-interaction-model\n",
       "\t    child.setAttribute('data-lm-suppress-shortcuts', 'true')\n",
       "\t  }\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "00ca0b8c-4b6b-4519-94a9-22e79bf1bbef"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "import utils\n",
    "\n",
    "import panel as pn  # GUI\n",
    "pn.extension()\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_completion_from_messages(messages, model=\"gpt-3.5-turbo\", temperature=0, max_tokens=500):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, \n",
    "        max_tokens=max_tokens, \n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System of chained prompts for processing the user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, illicit=None, illicit_violent=None, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False), category_applied_input_types=None, category_scores=CategoryScores(harassment=1.1170866855536588e-05, harassment_threatening=1.4300552493295982e-06, hate=6.180536047395435e-07, hate_threatening=8.955092312135093e-08, illicit=None, illicit_violent=None, self_harm=8.116802746371832e-06, self_harm_instructions=7.912258297437802e-06, self_harm_intent=6.722237594658509e-05, sexual=9.00048416951904e-06, sexual_minors=6.928541552042589e-06, violence=6.768796447431669e-05, violence_graphic=1.6808195368867018e-06, self-harm=8.116802746371832e-06, sexual/minors=6.928541552042589e-06, hate/threatening=8.955092312135093e-08, violence/graphic=1.6808195368867018e-06, self-harm/intent=6.722237594658509e-05, self-harm/instructions=7.912258297437802e-06, harassment/threatening=1.4300552493295982e-06), flagged=False)]\n",
      "Step 1: Input passed moderation check.\n",
      "Error: Invalid JSON string\n",
      "Step 2: Extracted list of products.\n",
      "Step 3: Looked up product information.\n",
      "Step 4: Generated response to user question.\n",
      "Step 6: Model evaluated the response.\n",
      "Step 7: Model approved the response.\n",
      "- The SmartX Pro phone is a high-performance smartphone with a powerful processor, advanced camera features, and a sleek design. It offers a great user experience for both work and entertainment.\n",
      "- The FotoSnap camera is a DSLR camera known for its professional-quality photos and versatile shooting capabilities. It's a great choice for photography enthusiasts and professionals.\n",
      "- Our range of TVs includes various sizes and features such as 4K resolution, smart TV capabilities, and HDR technology. They offer an immersive viewing experience for movies, sports, and gaming.\n",
      "\n",
      "Do you have any specific questions about the SmartX Pro phone, the FotoSnap camera, or our TVs? Are you looking for any particular features or specifications in these products?\n"
     ]
    }
   ],
   "source": [
    "def process_user_message(user_input, all_messages, debug=True):\n",
    "    delimiter = \"```\"\n",
    "    \n",
    "    # Step 1: Check input to see if it flags the Moderation API or is a prompt injection\n",
    "    response = openai.moderations.create(input=user_input)\n",
    "    moderation_output = response.results\n",
    "    print(moderation_output)\n",
    "    #Robin cHanged\n",
    "    if moderation_output[0].flagged:\n",
    "        print(\"Step 1: Input flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot process this request.\"\n",
    "\n",
    "    if debug: print(\"Step 1: Input passed moderation check.\")\n",
    "    category_and_product_response = utils.find_category_and_product_only(user_input, utils.get_products_and_category())\n",
    "    #print(print(category_and_product_response)\n",
    "    # Step 2: Extract the list of products\n",
    "    category_and_product_list = utils.read_string_to_list(category_and_product_response)\n",
    "    #print(category_and_product_list)\n",
    "\n",
    "    if debug: print(\"Step 2: Extracted list of products.\")\n",
    "\n",
    "    # Step 3: If products are found, look them up\n",
    "    product_information = utils.generate_output_string(category_and_product_list)\n",
    "    if debug: print(\"Step 3: Looked up product information.\")\n",
    "\n",
    "    # Step 4: Answer the user question\n",
    "    system_message = f\"\"\"\n",
    "    You are a customer service assistant for a large electronic store. \n",
    "    Respond in a friendly and helpful tone, with concise answers. \n",
    "    Make sure to ask the user relevant follow-up questions.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"},\n",
    "        {'role': 'assistant', 'content': f\"Relevant product information:\\n{product_information}\"}\n",
    "    ]\n",
    "\n",
    "    final_response = get_completion_from_messages(all_messages + messages)\n",
    "    if debug:print(\"Step 4: Generated response to user question.\")\n",
    "    all_messages = all_messages + messages[1:]\n",
    "\n",
    "    # Step 5: Put the answer through the Moderation API\n",
    "    response = openai.moderations.create(input=final_response)\n",
    "    moderation_output = response.results\n",
    "    # Robin changed\n",
    "    if moderation_output[0].flagged:\n",
    "        if debug: print(\"Step 5: Response flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot provide this information.\"\n",
    "\n",
    "    #if debug: print(\"Step 5: Response passed moderation check.\")\n",
    "\n",
    "    # Step 6: Ask the model if the response answers the initial user query well\n",
    "    user_message = f\"\"\"\n",
    "    Customer message: {delimiter}{user_input}{delimiter}\n",
    "    Agent response: {delimiter}{final_response}{delimiter}\n",
    "\n",
    "    Does the response sufficiently answer the question?\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "    evaluation_response = get_completion_from_messages(messages)\n",
    "    if debug: print(\"Step 6: Model evaluated the response.\")\n",
    "\n",
    "    # Step 7: If yes, use this answer; if not, say that you will connect the user to a human\n",
    "    if \"Y\" in evaluation_response:  # Using \"in\" instead of \"==\" to be safer for model output variation (e.g., \"Y.\" or \"Yes\")\n",
    "        if debug: print(\"Step 7: Model approved the response.\")\n",
    "        return final_response, all_messages\n",
    "    else:\n",
    "        if debug: print(\"Step 7: Model disapproved the response.\")\n",
    "        neg_str = \"I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\"\n",
    "        return neg_str, all_messages\n",
    "\n",
    "user_input = \"tell me about the smartx pro phone and the fotosnap camera, the dslr one. Also what tell me about your tvs\"\n",
    "response,_ = process_user_message(user_input,[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, illicit=None, illicit_violent=None, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False), category_applied_input_types=None, category_scores=CategoryScores(harassment=1.1170866855536588e-05, harassment_threatening=1.4300552493295982e-06, hate=6.180536047395435e-07, hate_threatening=8.955092312135093e-08, illicit=None, illicit_violent=None, self_harm=8.116802746371832e-06, self_harm_instructions=7.912258297437802e-06, self_harm_intent=6.722231773892418e-05, sexual=9.000466889119707e-06, sexual_minors=6.928541552042589e-06, violence=6.768796447431669e-05, violence_graphic=1.6808195368867018e-06, self-harm=8.116802746371832e-06, sexual/minors=6.928541552042589e-06, hate/threatening=8.955092312135093e-08, violence/graphic=1.6808195368867018e-06, self-harm/intent=6.722231773892418e-05, self-harm/instructions=7.912258297437802e-06, harassment/threatening=1.4300552493295982e-06), flagged=False)]\n",
      "Step 1: Input passed moderation check.\n",
      "Error: Invalid JSON string\n",
      "Step 2: Extracted list of products.\n",
      "Step 3: Looked up product information.\n",
      "Step 4: Generated response to user question.\n",
      "Step 6: Model evaluated the response.\n",
      "Step 7: Model approved the response.\n",
      "- The SmartX Pro phone is a high-performance smartphone with a powerful processor, advanced camera features, and a sleek design. It offers a great user experience for both work and entertainment.\n",
      "- The FotoSnap camera is a DSLR camera known for its professional-grade image quality, manual controls, and interchangeable lenses. It's perfect for photography enthusiasts and professionals.\n",
      "- Our range of TVs includes various sizes, resolutions, and smart features. We have options from leading brands that offer stunning picture quality, immersive sound, and smart functionalities like streaming services and voice control.\n",
      "\n",
      "Do you have any specific questions about the features, specifications, or pricing of these products? And are you looking for any particular TV size or feature for your home setup?\n"
     ]
    }
   ],
   "source": [
    "def process_user_message(user_input, all_messages, debug=True):\n",
    "    delimiter = \"```\"\n",
    "    \n",
    "    # Step 1: Check input to see if it flags the Moderation API or is a prompt injection\n",
    "    response = openai.moderations.create(input=user_input)\n",
    "    moderation_output = response.results\n",
    "    print(moderation_output)\n",
    "    #Robin cHanged\n",
    "    if moderation_output[0].flagged:\n",
    "        print(\"Step 1: Input flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot process this request.\"\n",
    "\n",
    "    if debug: print(\"Step 1: Input passed moderation check.\")\n",
    "    category_and_product_response = utils.find_category_and_product_only(user_input, utils.get_products_and_category())\n",
    "    #print(print(category_and_product_response)\n",
    "    # Step 2: Extract the list of products\n",
    "    category_and_product_list = utils.read_string_to_list(category_and_product_response)\n",
    "    #print(category_and_product_list)\n",
    "\n",
    "    if debug: print(\"Step 2: Extracted list of products.\")\n",
    "\n",
    "    # Step 3: If products are found, look them up\n",
    "    product_information = utils.generate_output_string(category_and_product_list)\n",
    "    if debug: print(\"Step 3: Looked up product information.\")\n",
    "\n",
    "    # Step 4: Answer the user question\n",
    "    system_message = f\"\"\"\n",
    "    You are a customer service assistant for a large electronic store. \n",
    "    Respond in a friendly and helpful tone, with concise answers. \n",
    "    Make sure to ask the user relevant follow-up questions.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"},\n",
    "        {'role': 'assistant', 'content': f\"Relevant product information:\\n{product_information}\"}\n",
    "    ]\n",
    "\n",
    "    final_response = get_completion_from_messages(all_messages + messages)\n",
    "    if debug:print(\"Step 4: Generated response to user question.\")\n",
    "    all_messages = all_messages + messages[1:]\n",
    "\n",
    "    # Step 5: Put the answer through the Moderation API\n",
    "    response = openai.moderations.create(input=final_response)\n",
    "    moderation_output = response.results\n",
    "    # Robin changed\n",
    "    if moderation_output[0].flagged:\n",
    "        if debug: print(\"Step 5: Response flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot provide this information.\"\n",
    "\n",
    "    #if debug: print(\"Step 5: Response passed moderation check.\")\n",
    "\n",
    "    # Step 6: Ask the model if the response answers the initial user query well\n",
    "    user_message = f\"\"\"\n",
    "    Customer message: {delimiter}{user_input}{delimiter}\n",
    "    Agent response: {delimiter}{final_response}{delimiter}\n",
    "\n",
    "    Does the response sufficiently answer the question?\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "    evaluation_response = get_completion_from_messages(messages)\n",
    "    if debug: print(\"Step 6: Model evaluated the response.\")\n",
    "\n",
    "    # Step 7: If yes, use this answer; if not, say that you will connect the user to a human\n",
    "    if \"Y\" in evaluation_response:  # Using \"in\" instead of \"==\" to be safer for model output variation (e.g., \"Y.\" or \"Yes\")\n",
    "        if debug: print(\"Step 7: Model approved the response.\")\n",
    "        return final_response, all_messages\n",
    "    else:\n",
    "        if debug: print(\"Step 7: Model disapproved the response.\")\n",
    "        neg_str = \"I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\"\n",
    "        return neg_str, all_messages\n",
    "\n",
    "user_input = \"tell me about the smartx pro phone and the fotosnap camera, the dslr one. Also what tell me about your tvs\"\n",
    "response,_ = process_user_message(user_input,[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, illicit=None, illicit_violent=None, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False), category_applied_input_types=None, category_scores=CategoryScores(harassment=1.1170866855536588e-05, harassment_threatening=1.4300552493295982e-06, hate=6.180536047395435e-07, hate_threatening=8.955092312135093e-08, illicit=None, illicit_violent=None, self_harm=8.116802746371832e-06, self_harm_instructions=7.912258297437802e-06, self_harm_intent=6.722237594658509e-05, sexual=9.00048416951904e-06, sexual_minors=6.928541552042589e-06, violence=6.768796447431669e-05, violence_graphic=1.6808195368867018e-06, self-harm=8.116802746371832e-06, sexual/minors=6.928541552042589e-06, hate/threatening=8.955092312135093e-08, violence/graphic=1.6808195368867018e-06, self-harm/intent=6.722237594658509e-05, self-harm/instructions=7.912258297437802e-06, harassment/threatening=1.4300552493295982e-06), flagged=False)]\n",
      "Step 1: Input passed moderation check.\n",
      "Error: Invalid JSON string\n",
      "Step 2: Extracted list of products.\n",
      "Step 3: Looked up product information.\n",
      "Step 4: Generated response to user question.\n",
      "Step 6: Model evaluated the response.\n",
      "Step 7: Model approved the response.\n",
      "- The SmartX Pro phone is a high-performance smartphone with a powerful processor, advanced camera features, and a sleek design. It offers a great user experience for both work and entertainment.\n",
      "- The FotoSnap camera is a DSLR camera known for its professional-grade image quality, manual controls, and interchangeable lenses. It's perfect for photography enthusiasts and professionals.\n",
      "- Our range of TVs includes various sizes, resolutions, and smart features. We have options from leading brands that offer stunning picture quality, immersive sound, and smart functionalities like streaming services and voice control.\n",
      "\n",
      "Do you have any specific questions about the features, specifications, or pricing of these products?\n"
     ]
    }
   ],
   "source": [
    "def process_user_message(user_input, all_messages, debug=True):\n",
    "    delimiter = \"```\"\n",
    "    \n",
    "    # Step 1: Check input to see if it flags the Moderation API or is a prompt injection\n",
    "    response = openai.moderations.create(input=user_input)\n",
    "    moderation_output = response.results\n",
    "    print(moderation_output)\n",
    "    #Robin cHanged\n",
    "    if moderation_output[0].flagged:\n",
    "        print(\"Step 1: Input flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot process this request.\"\n",
    "\n",
    "    if debug: print(\"Step 1: Input passed moderation check.\")\n",
    "    category_and_product_response = utils.find_category_and_product_only(user_input, utils.get_products_and_category())\n",
    "    #print(print(category_and_product_response)\n",
    "    # Step 2: Extract the list of products\n",
    "    category_and_product_list = utils.read_string_to_list(category_and_product_response)\n",
    "    #print(category_and_product_list)\n",
    "\n",
    "    if debug: print(\"Step 2: Extracted list of products.\")\n",
    "\n",
    "    # Step 3: If products are found, look them up\n",
    "    product_information = utils.generate_output_string(category_and_product_list)\n",
    "    if debug: print(\"Step 3: Looked up product information.\")\n",
    "\n",
    "    # Step 4: Answer the user question\n",
    "    system_message = f\"\"\"\n",
    "    You are a customer service assistant for a large electronic store. \n",
    "    Respond in a friendly and helpful tone, with concise answers. \n",
    "    Make sure to ask the user relevant follow-up questions.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"},\n",
    "        {'role': 'assistant', 'content': f\"Relevant product information:\\n{product_information}\"}\n",
    "    ]\n",
    "\n",
    "    final_response = get_completion_from_messages(all_messages + messages)\n",
    "    if debug:print(\"Step 4: Generated response to user question.\")\n",
    "    all_messages = all_messages + messages[1:]\n",
    "\n",
    "    # Step 5: Put the answer through the Moderation API\n",
    "    response = openai.moderations.create(input=final_response)\n",
    "    moderation_output = response.results\n",
    "    # Robin changed\n",
    "    if moderation_output[0].flagged:\n",
    "        if debug: print(\"Step 5: Response flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot provide this information.\"\n",
    "\n",
    "    #if debug: print(\"Step 5: Response passed moderation check.\")\n",
    "\n",
    "    # Step 6: Ask the model if the response answers the initial user query well\n",
    "    user_message = f\"\"\"\n",
    "    Customer message: {delimiter}{user_input}{delimiter}\n",
    "    Agent response: {delimiter}{final_response}{delimiter}\n",
    "\n",
    "    Does the response sufficiently answer the question?\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "    evaluation_response = get_completion_from_messages(messages)\n",
    "    if debug: print(\"Step 6: Model evaluated the response.\")\n",
    "\n",
    "    # Step 7: If yes, use this answer; if not, say that you will connect the user to a human\n",
    "    if \"Y\" in evaluation_response:  # Using \"in\" instead of \"==\" to be safer for model output variation (e.g., \"Y.\" or \"Yes\")\n",
    "        if debug: print(\"Step 7: Model approved the response.\")\n",
    "        return final_response, all_messages\n",
    "    else:\n",
    "        if debug: print(\"Step 7: Model disapproved the response.\")\n",
    "        neg_str = \"I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\"\n",
    "        return neg_str, all_messages\n",
    "\n",
    "user_input = \"tell me about the smartx pro phone and the fotosnap camera, the dslr one. Also what tell me about your tvs\"\n",
    "response,_ = process_user_message(user_input,[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, illicit=None, illicit_violent=None, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False), category_applied_input_types=None, category_scores=CategoryScores(harassment=1.1170866855536588e-05, harassment_threatening=1.4300552493295982e-06, hate=6.180536047395435e-07, hate_threatening=8.955092312135093e-08, illicit=None, illicit_violent=None, self_harm=8.116802746371832e-06, self_harm_instructions=7.912258297437802e-06, self_harm_intent=6.722237594658509e-05, sexual=9.00048416951904e-06, sexual_minors=6.928541552042589e-06, violence=6.768796447431669e-05, violence_graphic=1.6808195368867018e-06, self-harm=8.116802746371832e-06, sexual/minors=6.928541552042589e-06, hate/threatening=8.955092312135093e-08, violence/graphic=1.6808195368867018e-06, self-harm/intent=6.722237594658509e-05, self-harm/instructions=7.912258297437802e-06, harassment/threatening=1.4300552493295982e-06), flagged=False)]\n",
      "Step 1: Input passed moderation check.\n",
      "Error: Invalid JSON string\n",
      "Step 2: Extracted list of products.\n",
      "Step 3: Looked up product information.\n",
      "Step 4: Generated response to user question.\n",
      "Step 6: Model evaluated the response.\n",
      "Step 7: Model approved the response.\n",
      "- The SmartX Pro phone is a high-performance smartphone with a powerful processor, advanced camera features, and a sleek design. It offers a great user experience for both work and entertainment.\n",
      "- The Fotosnap camera is a DSLR camera known for its high-quality images, interchangeable lenses, and manual controls. It's perfect for photography enthusiasts and professionals looking to capture stunning photos.\n",
      "- Our range of TVs includes various sizes, resolutions, and smart features. We have options from leading brands that offer excellent picture quality, smart connectivity, and immersive viewing experiences. \n",
      "\n",
      "Do you have any specific questions about the features or specifications of these products? Are you looking for a particular model or feature in a TV?\n"
     ]
    }
   ],
   "source": [
    "def process_user_message(user_input, all_messages, debug=True):\n",
    "    delimiter = \"```\"\n",
    "    \n",
    "    # Step 1: Check input to see if it flags the Moderation API or is a prompt injection\n",
    "    response = openai.moderations.create(input=user_input)\n",
    "    moderation_output = response.results\n",
    "    print(moderation_output)\n",
    "    #Robin cHanged\n",
    "    if moderation_output[0].flagged:\n",
    "        print(\"Step 1: Input flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot process this request.\"\n",
    "\n",
    "    if debug: print(\"Step 1: Input passed moderation check.\")\n",
    "    category_and_product_response = utils.find_category_and_product_only(user_input, utils.get_products_and_category())\n",
    "    #print(print(category_and_product_response)\n",
    "    # Step 2: Extract the list of products\n",
    "    category_and_product_list = utils.read_string_to_list(category_and_product_response)\n",
    "    #print(category_and_product_list)\n",
    "\n",
    "    if debug: print(\"Step 2: Extracted list of products.\")\n",
    "\n",
    "    # Step 3: If products are found, look them up\n",
    "    product_information = utils.generate_output_string(category_and_product_list)\n",
    "    if debug: print(\"Step 3: Looked up product information.\")\n",
    "\n",
    "    # Step 4: Answer the user question\n",
    "    system_message = f\"\"\"\n",
    "    You are a customer service assistant for a large electronic store. \n",
    "    Respond in a friendly and helpful tone, with concise answers. \n",
    "    Make sure to ask the user relevant follow-up questions.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"},\n",
    "        {'role': 'assistant', 'content': f\"Relevant product information:\\n{product_information}\"}\n",
    "    ]\n",
    "\n",
    "    final_response = get_completion_from_messages(all_messages + messages)\n",
    "    if debug:print(\"Step 4: Generated response to user question.\")\n",
    "    all_messages = all_messages + messages[1:]\n",
    "\n",
    "    # Step 5: Put the answer through the Moderation API\n",
    "    response = openai.moderations.create(input=final_response)\n",
    "    moderation_output = response.results\n",
    "    # Robin changed\n",
    "    if moderation_output[0].flagged:\n",
    "        if debug: print(\"Step 5: Response flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot provide this information.\"\n",
    "\n",
    "    #if debug: print(\"Step 5: Response passed moderation check.\")\n",
    "\n",
    "    # Step 6: Ask the model if the response answers the initial user query well\n",
    "    user_message = f\"\"\"\n",
    "    Customer message: {delimiter}{user_input}{delimiter}\n",
    "    Agent response: {delimiter}{final_response}{delimiter}\n",
    "\n",
    "    Does the response sufficiently answer the question?\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "    evaluation_response = get_completion_from_messages(messages)\n",
    "    if debug: print(\"Step 6: Model evaluated the response.\")\n",
    "\n",
    "    # Step 7: If yes, use this answer; if not, say that you will connect the user to a human\n",
    "    if \"Y\" in evaluation_response:  # Using \"in\" instead of \"==\" to be safer for model output variation (e.g., \"Y.\" or \"Yes\")\n",
    "        if debug: print(\"Step 7: Model approved the response.\")\n",
    "        return final_response, all_messages\n",
    "    else:\n",
    "        if debug: print(\"Step 7: Model disapproved the response.\")\n",
    "        neg_str = \"I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\"\n",
    "        return neg_str, all_messages\n",
    "\n",
    "user_input = \"tell me about the smartx pro phone and the fotosnap camera, the dslr one. Also what tell me about your tvs\"\n",
    "response,_ = process_user_message(user_input,[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, illicit=None, illicit_violent=None, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False), category_applied_input_types=None, category_scores=CategoryScores(harassment=1.1170866855536588e-05, harassment_threatening=1.4300552493295982e-06, hate=6.180536047395435e-07, hate_threatening=8.955092312135093e-08, illicit=None, illicit_violent=None, self_harm=8.116802746371832e-06, self_harm_instructions=7.912258297437802e-06, self_harm_intent=6.722237594658509e-05, sexual=9.00048416951904e-06, sexual_minors=6.928541552042589e-06, violence=6.768796447431669e-05, violence_graphic=1.6808195368867018e-06, self-harm=8.116802746371832e-06, sexual/minors=6.928541552042589e-06, hate/threatening=8.955092312135093e-08, violence/graphic=1.6808195368867018e-06, self-harm/intent=6.722237594658509e-05, self-harm/instructions=7.912258297437802e-06, harassment/threatening=1.4300552493295982e-06), flagged=False)]\n",
      "Step 1: Input passed moderation check.\n",
      "Error: Invalid JSON string\n",
      "Step 2: Extracted list of products.\n",
      "Step 3: Looked up product information.\n",
      "Step 4: Generated response to user question.\n",
      "Step 6: Model evaluated the response.\n",
      "Step 7: Model approved the response.\n",
      "- The SmartX Pro phone is a high-performance smartphone with a powerful processor, advanced camera features, and a sleek design. It offers a great user experience for both work and entertainment.\n",
      "- The FotoSnap camera is a DSLR camera known for its professional-grade image quality, manual controls, and interchangeable lenses. It's perfect for photography enthusiasts and professionals.\n",
      "- Our range of TVs includes various sizes, resolutions, and smart features. We have options from leading brands that offer stunning picture quality, immersive sound, and smart functionalities like streaming services and voice control.\n",
      "\n",
      "Do you have any specific questions about the features, specifications, or pricing of these products?\n"
     ]
    }
   ],
   "source": [
    "def process_user_message(user_input, all_messages, debug=True):\n",
    "    delimiter = \"```\"\n",
    "    \n",
    "    # Step 1: Check input to see if it flags the Moderation API or is a prompt injection\n",
    "    response = openai.moderations.create(input=user_input)\n",
    "    moderation_output = response.results\n",
    "    print(moderation_output)\n",
    "    #Robin cHanged\n",
    "    if moderation_output[0].flagged:\n",
    "        print(\"Step 1: Input flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot process this request.\"\n",
    "\n",
    "    if debug: print(\"Step 1: Input passed moderation check.\")\n",
    "    category_and_product_response = utils.find_category_and_product_only(user_input, utils.get_products_and_category())\n",
    "    #print(print(category_and_product_response)\n",
    "    # Step 2: Extract the list of products\n",
    "    category_and_product_list = utils.read_string_to_list(category_and_product_response)\n",
    "    #print(category_and_product_list)\n",
    "\n",
    "    if debug: print(\"Step 2: Extracted list of products.\")\n",
    "\n",
    "    # Step 3: If products are found, look them up\n",
    "    product_information = utils.generate_output_string(category_and_product_list)\n",
    "    if debug: print(\"Step 3: Looked up product information.\")\n",
    "\n",
    "    # Step 4: Answer the user question\n",
    "    system_message = f\"\"\"\n",
    "    You are a customer service assistant for a large electronic store. \n",
    "    Respond in a friendly and helpful tone, with concise answers. \n",
    "    Make sure to ask the user relevant follow-up questions.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"},\n",
    "        {'role': 'assistant', 'content': f\"Relevant product information:\\n{product_information}\"}\n",
    "    ]\n",
    "\n",
    "    final_response = get_completion_from_messages(all_messages + messages)\n",
    "    if debug:print(\"Step 4: Generated response to user question.\")\n",
    "    all_messages = all_messages + messages[1:]\n",
    "\n",
    "    # Step 5: Put the answer through the Moderation API\n",
    "    response = openai.moderations.create(input=final_response)\n",
    "    moderation_output = response.results\n",
    "    # Robin changed\n",
    "    if moderation_output[0].flagged:\n",
    "        if debug: print(\"Step 5: Response flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot provide this information.\"\n",
    "\n",
    "    #if debug: print(\"Step 5: Response passed moderation check.\")\n",
    "\n",
    "    # Step 6: Ask the model if the response answers the initial user query well\n",
    "    user_message = f\"\"\"\n",
    "    Customer message: {delimiter}{user_input}{delimiter}\n",
    "    Agent response: {delimiter}{final_response}{delimiter}\n",
    "\n",
    "    Does the response sufficiently answer the question?\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "    evaluation_response = get_completion_from_messages(messages)\n",
    "    if debug: print(\"Step 6: Model evaluated the response.\")\n",
    "\n",
    "    # Step 7: If yes, use this answer; if not, say that you will connect the user to a human\n",
    "    if \"Y\" in evaluation_response:  # Using \"in\" instead of \"==\" to be safer for model output variation (e.g., \"Y.\" or \"Yes\")\n",
    "        if debug: print(\"Step 7: Model approved the response.\")\n",
    "        return final_response, all_messages\n",
    "    else:\n",
    "        if debug: print(\"Step 7: Model disapproved the response.\")\n",
    "        neg_str = \"I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\"\n",
    "        return neg_str, all_messages\n",
    "\n",
    "user_input = \"tell me about the smartx pro phone and the fotosnap camera, the dslr one. Also what tell me about your tvs\"\n",
    "response,_ = process_user_message(user_input,[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, illicit=None, illicit_violent=None, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False), category_applied_input_types=None, category_scores=CategoryScores(harassment=1.1170866855536588e-05, harassment_threatening=1.4300552493295982e-06, hate=6.180536047395435e-07, hate_threatening=8.955092312135093e-08, illicit=None, illicit_violent=None, self_harm=8.116802746371832e-06, self_harm_instructions=7.912258297437802e-06, self_harm_intent=6.722237594658509e-05, sexual=9.00048416951904e-06, sexual_minors=6.928541552042589e-06, violence=6.768796447431669e-05, violence_graphic=1.6808195368867018e-06, self-harm=8.116802746371832e-06, sexual/minors=6.928541552042589e-06, hate/threatening=8.955092312135093e-08, violence/graphic=1.6808195368867018e-06, self-harm/intent=6.722237594658509e-05, self-harm/instructions=7.912258297437802e-06, harassment/threatening=1.4300552493295982e-06), flagged=False)]\n",
      "Step 1: Input passed moderation check.\n",
      "Error: Invalid JSON string\n",
      "Step 2: Extracted list of products.\n",
      "Step 3: Looked up product information.\n",
      "Step 4: Generated response to user question.\n",
      "Step 6: Model evaluated the response.\n",
      "Step 7: Model approved the response.\n",
      "- The SmartX Pro phone is a high-performance smartphone with a powerful processor, advanced camera features, and a sleek design. It offers a large display and long battery life for all-day use. Do you have any specific questions about its features or specifications?\n",
      "\n",
      "- The FotoSnap camera is a DSLR camera known for its professional-quality photos and versatility. It has interchangeable lenses, manual controls, and advanced shooting modes for creative photography. Are you looking for any specific details about its resolution or features?\n",
      "\n",
      "- Our range of TVs includes various sizes, resolutions, and smart features to suit different preferences. We have 4K Ultra HD TVs, Smart TVs with built-in streaming services, and OLED displays for stunning picture quality. Are you interested in a particular size or feature for your TV?\n"
     ]
    }
   ],
   "source": [
    "def process_user_message(user_input, all_messages, debug=True):\n",
    "    delimiter = \"```\"\n",
    "    \n",
    "    # Step 1: Check input to see if it flags the Moderation API or is a prompt injection\n",
    "    response = openai.moderations.create(input=user_input)\n",
    "    moderation_output = response.results\n",
    "    print(moderation_output)\n",
    "    #Robin cHanged\n",
    "    if moderation_output[0].flagged:\n",
    "        print(\"Step 1: Input flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot process this request.\"\n",
    "\n",
    "    if debug: print(\"Step 1: Input passed moderation check.\")\n",
    "    category_and_product_response = utils.find_category_and_product_only(user_input, utils.get_products_and_category())\n",
    "    #print(print(category_and_product_response)\n",
    "    # Step 2: Extract the list of products\n",
    "    category_and_product_list = utils.read_string_to_list(category_and_product_response)\n",
    "    #print(category_and_product_list)\n",
    "\n",
    "    if debug: print(\"Step 2: Extracted list of products.\")\n",
    "\n",
    "    # Step 3: If products are found, look them up\n",
    "    product_information = utils.generate_output_string(category_and_product_list)\n",
    "    if debug: print(\"Step 3: Looked up product information.\")\n",
    "\n",
    "    # Step 4: Answer the user question\n",
    "    system_message = f\"\"\"\n",
    "    You are a customer service assistant for a large electronic store. \n",
    "    Respond in a friendly and helpful tone, with concise answers. \n",
    "    Make sure to ask the user relevant follow-up questions.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"},\n",
    "        {'role': 'assistant', 'content': f\"Relevant product information:\\n{product_information}\"}\n",
    "    ]\n",
    "\n",
    "    final_response = get_completion_from_messages(all_messages + messages)\n",
    "    if debug:print(\"Step 4: Generated response to user question.\")\n",
    "    all_messages = all_messages + messages[1:]\n",
    "\n",
    "    # Step 5: Put the answer through the Moderation API\n",
    "    response = openai.moderations.create(input=final_response)\n",
    "    moderation_output = response.results\n",
    "    # Robin changed\n",
    "    if moderation_output[0].flagged:\n",
    "        if debug: print(\"Step 5: Response flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot provide this information.\"\n",
    "\n",
    "    #if debug: print(\"Step 5: Response passed moderation check.\")\n",
    "\n",
    "    # Step 6: Ask the model if the response answers the initial user query well\n",
    "    user_message = f\"\"\"\n",
    "    Customer message: {delimiter}{user_input}{delimiter}\n",
    "    Agent response: {delimiter}{final_response}{delimiter}\n",
    "\n",
    "    Does the response sufficiently answer the question?\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "    evaluation_response = get_completion_from_messages(messages)\n",
    "    if debug: print(\"Step 6: Model evaluated the response.\")\n",
    "\n",
    "    # Step 7: If yes, use this answer; if not, say that you will connect the user to a human\n",
    "    if \"Y\" in evaluation_response:  # Using \"in\" instead of \"==\" to be safer for model output variation (e.g., \"Y.\" or \"Yes\")\n",
    "        if debug: print(\"Step 7: Model approved the response.\")\n",
    "        return final_response, all_messages\n",
    "    else:\n",
    "        if debug: print(\"Step 7: Model disapproved the response.\")\n",
    "        neg_str = \"I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\"\n",
    "        return neg_str, all_messages\n",
    "\n",
    "user_input = \"tell me about the smartx pro phone and the fotosnap camera, the dslr one. Also what tell me about your tvs\"\n",
    "response,_ = process_user_message(user_input,[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, illicit=None, illicit_violent=None, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False), category_applied_input_types=None, category_scores=CategoryScores(harassment=1.1170866855536588e-05, harassment_threatening=1.4300552493295982e-06, hate=6.180536047395435e-07, hate_threatening=8.955092312135093e-08, illicit=None, illicit_violent=None, self_harm=8.116802746371832e-06, self_harm_instructions=7.912258297437802e-06, self_harm_intent=6.722237594658509e-05, sexual=9.00048416951904e-06, sexual_minors=6.928541552042589e-06, violence=6.768796447431669e-05, violence_graphic=1.6808195368867018e-06, self-harm=8.116802746371832e-06, sexual/minors=6.928541552042589e-06, hate/threatening=8.955092312135093e-08, violence/graphic=1.6808195368867018e-06, self-harm/intent=6.722237594658509e-05, self-harm/instructions=7.912258297437802e-06, harassment/threatening=1.4300552493295982e-06), flagged=False)]\n",
      "Step 1: Input passed moderation check.\n",
      "Error: Invalid JSON string\n",
      "Step 2: Extracted list of products.\n",
      "Step 3: Looked up product information.\n",
      "Step 4: Generated response to user question.\n",
      "Step 6: Model evaluated the response.\n",
      "Step 7: Model approved the response.\n",
      "- The SmartX Pro phone is a high-performance smartphone with a powerful processor, advanced camera features, and a sleek design. It offers a smooth user experience and great connectivity options. Do you have any specific questions about its features or specifications?\n",
      "\n",
      "- The FotoSnap camera is a DSLR camera known for its professional-quality photos and versatile shooting capabilities. It offers manual controls for advanced users and automatic modes for beginners. Are you looking for any specific details about its resolution, lens options, or shooting modes?\n",
      "\n",
      "- Our range of TVs includes various sizes, resolutions, and smart features to suit different preferences. We have 4K Ultra HD TVs, Smart TVs with built-in streaming services, and OLED TVs for stunning picture quality. Are you interested in a particular size or feature for your TV?\n"
     ]
    }
   ],
   "source": [
    "def process_user_message(user_input, all_messages, debug=True):\n",
    "    delimiter = \"```\"\n",
    "    \n",
    "    # Step 1: Check input to see if it flags the Moderation API or is a prompt injection\n",
    "    response = openai.moderations.create(input=user_input)\n",
    "    moderation_output = response.results\n",
    "    print(moderation_output)\n",
    "    #Robin cHanged\n",
    "    if moderation_output[0].flagged:\n",
    "        print(\"Step 1: Input flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot process this request.\"\n",
    "\n",
    "    if debug: print(\"Step 1: Input passed moderation check.\")\n",
    "    category_and_product_response = utils.find_category_and_product_only(user_input, utils.get_products_and_category())\n",
    "    #print(print(category_and_product_response)\n",
    "    # Step 2: Extract the list of products\n",
    "    category_and_product_list = utils.read_string_to_list(category_and_product_response)\n",
    "    #print(category_and_product_list)\n",
    "\n",
    "    if debug: print(\"Step 2: Extracted list of products.\")\n",
    "\n",
    "    # Step 3: If products are found, look them up\n",
    "    product_information = utils.generate_output_string(category_and_product_list)\n",
    "    if debug: print(\"Step 3: Looked up product information.\")\n",
    "\n",
    "    # Step 4: Answer the user question\n",
    "    system_message = f\"\"\"\n",
    "    You are a customer service assistant for a large electronic store. \n",
    "    Respond in a friendly and helpful tone, with concise answers. \n",
    "    Make sure to ask the user relevant follow-up questions.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"},\n",
    "        {'role': 'assistant', 'content': f\"Relevant product information:\\n{product_information}\"}\n",
    "    ]\n",
    "\n",
    "    final_response = get_completion_from_messages(all_messages + messages)\n",
    "    if debug:print(\"Step 4: Generated response to user question.\")\n",
    "    all_messages = all_messages + messages[1:]\n",
    "\n",
    "    # Step 5: Put the answer through the Moderation API\n",
    "    response = openai.moderations.create(input=final_response)\n",
    "    moderation_output = response.results\n",
    "    # Robin changed\n",
    "    if moderation_output[0].flagged:\n",
    "        if debug: print(\"Step 5: Response flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot provide this information.\"\n",
    "\n",
    "    #if debug: print(\"Step 5: Response passed moderation check.\")\n",
    "\n",
    "    # Step 6: Ask the model if the response answers the initial user query well\n",
    "    user_message = f\"\"\"\n",
    "    Customer message: {delimiter}{user_input}{delimiter}\n",
    "    Agent response: {delimiter}{final_response}{delimiter}\n",
    "\n",
    "    Does the response sufficiently answer the question?\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "    evaluation_response = get_completion_from_messages(messages)\n",
    "    if debug: print(\"Step 6: Model evaluated the response.\")\n",
    "\n",
    "    # Step 7: If yes, use this answer; if not, say that you will connect the user to a human\n",
    "    if \"Y\" in evaluation_response:  # Using \"in\" instead of \"==\" to be safer for model output variation (e.g., \"Y.\" or \"Yes\")\n",
    "        if debug: print(\"Step 7: Model approved the response.\")\n",
    "        return final_response, all_messages\n",
    "    else:\n",
    "        if debug: print(\"Step 7: Model disapproved the response.\")\n",
    "        neg_str = \"I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\"\n",
    "        return neg_str, all_messages\n",
    "\n",
    "user_input = \"tell me about the smartx pro phone and the fotosnap camera, the dslr one. Also what tell me about your tvs\"\n",
    "response,_ = process_user_message(user_input,[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, illicit=None, illicit_violent=None, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False), category_applied_input_types=None, category_scores=CategoryScores(harassment=1.1170866855536588e-05, harassment_threatening=1.4300552493295982e-06, hate=6.180536047395435e-07, hate_threatening=8.955092312135093e-08, illicit=None, illicit_violent=None, self_harm=8.116802746371832e-06, self_harm_instructions=7.912258297437802e-06, self_harm_intent=6.722237594658509e-05, sexual=9.00048416951904e-06, sexual_minors=6.928541552042589e-06, violence=6.768796447431669e-05, violence_graphic=1.6808195368867018e-06, self-harm=8.116802746371832e-06, sexual/minors=6.928541552042589e-06, hate/threatening=8.955092312135093e-08, violence/graphic=1.6808195368867018e-06, self-harm/intent=6.722237594658509e-05, self-harm/instructions=7.912258297437802e-06, harassment/threatening=1.4300552493295982e-06), flagged=False)]\n",
      "Step 1: Input passed moderation check.\n",
      "Error: Invalid JSON string\n",
      "Step 2: Extracted list of products.\n",
      "Step 3: Looked up product information.\n",
      "Step 4: Generated response to user question.\n",
      "Step 6: Model evaluated the response.\n",
      "Step 7: Model approved the response.\n",
      "- The SmartX Pro phone is a high-performance smartphone with a powerful processor, advanced camera features, and a sleek design. It offers a great user experience for both work and entertainment.\n",
      "- The FotoSnap camera is a DSLR camera known for its professional-grade image quality, manual controls, and versatility. It's perfect for photography enthusiasts and professionals looking to capture stunning photos.\n",
      "- Our range of TVs includes various sizes, resolutions, and smart features to suit different needs. From 4K Ultra HD to OLED displays, we have options for immersive viewing experiences.\n",
      "\n",
      "Do you have any specific questions about the SmartX Pro phone, the FotoSnap camera, or our TVs? Are you looking for any particular features or specifications in these products?\n"
     ]
    }
   ],
   "source": [
    "def process_user_message(user_input, all_messages, debug=True):\n",
    "    delimiter = \"```\"\n",
    "    \n",
    "    # Step 1: Check input to see if it flags the Moderation API or is a prompt injection\n",
    "    response = openai.moderations.create(input=user_input)\n",
    "    moderation_output = response.results\n",
    "    print(moderation_output)\n",
    "    #Robin cHanged\n",
    "    if moderation_output[0].flagged:\n",
    "        print(\"Step 1: Input flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot process this request.\"\n",
    "\n",
    "    if debug: print(\"Step 1: Input passed moderation check.\")\n",
    "    category_and_product_response = utils.find_category_and_product_only(user_input, utils.get_products_and_category())\n",
    "    #print(print(category_and_product_response)\n",
    "    # Step 2: Extract the list of products\n",
    "    category_and_product_list = utils.read_string_to_list(category_and_product_response)\n",
    "    #print(category_and_product_list)\n",
    "\n",
    "    if debug: print(\"Step 2: Extracted list of products.\")\n",
    "\n",
    "    # Step 3: If products are found, look them up\n",
    "    product_information = utils.generate_output_string(category_and_product_list)\n",
    "    if debug: print(\"Step 3: Looked up product information.\")\n",
    "\n",
    "    # Step 4: Answer the user question\n",
    "    system_message = f\"\"\"\n",
    "    You are a customer service assistant for a large electronic store. \n",
    "    Respond in a friendly and helpful tone, with concise answers. \n",
    "    Make sure to ask the user relevant follow-up questions.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"},\n",
    "        {'role': 'assistant', 'content': f\"Relevant product information:\\n{product_information}\"}\n",
    "    ]\n",
    "\n",
    "    final_response = get_completion_from_messages(all_messages + messages)\n",
    "    if debug:print(\"Step 4: Generated response to user question.\")\n",
    "    all_messages = all_messages + messages[1:]\n",
    "\n",
    "    # Step 5: Put the answer through the Moderation API\n",
    "    response = openai.moderations.create(input=final_response)\n",
    "    moderation_output = response.results\n",
    "    # Robin changed\n",
    "    if moderation_output[0].flagged:\n",
    "        if debug: print(\"Step 5: Response flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot provide this information.\"\n",
    "\n",
    "    #if debug: print(\"Step 5: Response passed moderation check.\")\n",
    "\n",
    "    # Step 6: Ask the model if the response answers the initial user query well\n",
    "    user_message = f\"\"\"\n",
    "    Customer message: {delimiter}{user_input}{delimiter}\n",
    "    Agent response: {delimiter}{final_response}{delimiter}\n",
    "\n",
    "    Does the response sufficiently answer the question?\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "    evaluation_response = get_completion_from_messages(messages)\n",
    "    if debug: print(\"Step 6: Model evaluated the response.\")\n",
    "\n",
    "    # Step 7: If yes, use this answer; if not, say that you will connect the user to a human\n",
    "    if \"Y\" in evaluation_response:  # Using \"in\" instead of \"==\" to be safer for model output variation (e.g., \"Y.\" or \"Yes\")\n",
    "        if debug: print(\"Step 7: Model approved the response.\")\n",
    "        return final_response, all_messages\n",
    "    else:\n",
    "        if debug: print(\"Step 7: Model disapproved the response.\")\n",
    "        neg_str = \"I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\"\n",
    "        return neg_str, all_messages\n",
    "\n",
    "user_input = \"tell me about the smartx pro phone and the fotosnap camera, the dslr one. Also what tell me about your tvs\"\n",
    "response,_ = process_user_message(user_input,[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, illicit=None, illicit_violent=None, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False), category_applied_input_types=None, category_scores=CategoryScores(harassment=1.1170866855536588e-05, harassment_threatening=1.4300552493295982e-06, hate=6.180536047395435e-07, hate_threatening=8.955092312135093e-08, illicit=None, illicit_violent=None, self_harm=8.116802746371832e-06, self_harm_instructions=7.912258297437802e-06, self_harm_intent=6.722237594658509e-05, sexual=9.00048416951904e-06, sexual_minors=6.928541552042589e-06, violence=6.768796447431669e-05, violence_graphic=1.6808195368867018e-06, self-harm=8.116802746371832e-06, sexual/minors=6.928541552042589e-06, hate/threatening=8.955092312135093e-08, violence/graphic=1.6808195368867018e-06, self-harm/intent=6.722237594658509e-05, self-harm/instructions=7.912258297437802e-06, harassment/threatening=1.4300552493295982e-06), flagged=False)]\n",
      "Step 1: Input passed moderation check.\n",
      "Error: Invalid JSON string\n",
      "Step 2: Extracted list of products.\n",
      "Step 3: Looked up product information.\n",
      "Step 4: Generated response to user question.\n",
      "Step 6: Model evaluated the response.\n",
      "Step 7: Model approved the response.\n",
      "- The SmartX Pro phone is a high-performance smartphone with a powerful processor, advanced camera features, and a sleek design. It offers a great user experience for both work and entertainment.\n",
      "- The FotoSnap camera is a DSLR camera known for its professional-grade image quality, manual controls, and versatility. It's perfect for photography enthusiasts and professionals looking to capture stunning photos.\n",
      "- Our range of TVs includes various sizes, resolutions, and smart features to suit different preferences. From 4K Ultra HD to OLED displays, we have options for immersive viewing experiences.\n",
      "\n",
      "Do you have any specific questions about the features, specifications, or pricing of these products? How can I assist you further in finding the right device for your needs?\n"
     ]
    }
   ],
   "source": [
    "def process_user_message(user_input, all_messages, debug=True):\n",
    "    delimiter = \"```\"\n",
    "    \n",
    "    # Step 1: Check input to see if it flags the Moderation API or is a prompt injection\n",
    "    response = openai.moderations.create(input=user_input)\n",
    "    moderation_output = response.results\n",
    "    print(moderation_output)\n",
    "    #Robin cHanged\n",
    "    if moderation_output[0].flagged:\n",
    "        print(\"Step 1: Input flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot process this request.\"\n",
    "\n",
    "    if debug: print(\"Step 1: Input passed moderation check.\")\n",
    "    category_and_product_response = utils.find_category_and_product_only(user_input, utils.get_products_and_category())\n",
    "    #print(print(category_and_product_response)\n",
    "    # Step 2: Extract the list of products\n",
    "    category_and_product_list = utils.read_string_to_list(category_and_product_response)\n",
    "    #print(category_and_product_list)\n",
    "\n",
    "    if debug: print(\"Step 2: Extracted list of products.\")\n",
    "\n",
    "    # Step 3: If products are found, look them up\n",
    "    product_information = utils.generate_output_string(category_and_product_list)\n",
    "    if debug: print(\"Step 3: Looked up product information.\")\n",
    "\n",
    "    # Step 4: Answer the user question\n",
    "    system_message = f\"\"\"\n",
    "    You are a customer service assistant for a large electronic store. \n",
    "    Respond in a friendly and helpful tone, with concise answers. \n",
    "    Make sure to ask the user relevant follow-up questions.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"},\n",
    "        {'role': 'assistant', 'content': f\"Relevant product information:\\n{product_information}\"}\n",
    "    ]\n",
    "\n",
    "    final_response = get_completion_from_messages(all_messages + messages)\n",
    "    if debug:print(\"Step 4: Generated response to user question.\")\n",
    "    all_messages = all_messages + messages[1:]\n",
    "\n",
    "    # Step 5: Put the answer through the Moderation API\n",
    "    response = openai.moderations.create(input=final_response)\n",
    "    moderation_output = response.results\n",
    "    # Robin changed\n",
    "    if moderation_output[0].flagged:\n",
    "        if debug: print(\"Step 5: Response flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot provide this information.\"\n",
    "\n",
    "    #if debug: print(\"Step 5: Response passed moderation check.\")\n",
    "\n",
    "    # Step 6: Ask the model if the response answers the initial user query well\n",
    "    user_message = f\"\"\"\n",
    "    Customer message: {delimiter}{user_input}{delimiter}\n",
    "    Agent response: {delimiter}{final_response}{delimiter}\n",
    "\n",
    "    Does the response sufficiently answer the question?\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "    evaluation_response = get_completion_from_messages(messages)\n",
    "    if debug: print(\"Step 6: Model evaluated the response.\")\n",
    "\n",
    "    # Step 7: If yes, use this answer; if not, say that you will connect the user to a human\n",
    "    if \"Y\" in evaluation_response:  # Using \"in\" instead of \"==\" to be safer for model output variation (e.g., \"Y.\" or \"Yes\")\n",
    "        if debug: print(\"Step 7: Model approved the response.\")\n",
    "        return final_response, all_messages\n",
    "    else:\n",
    "        if debug: print(\"Step 7: Model disapproved the response.\")\n",
    "        neg_str = \"I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\"\n",
    "        return neg_str, all_messages\n",
    "\n",
    "user_input = \"tell me about the smartx pro phone and the fotosnap camera, the dslr one. Also what tell me about your tvs\"\n",
    "response,_ = process_user_message(user_input,[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, illicit=None, illicit_violent=None, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False), category_applied_input_types=None, category_scores=CategoryScores(harassment=1.1170866855536588e-05, harassment_threatening=1.4300552493295982e-06, hate=6.180536047395435e-07, hate_threatening=8.955092312135093e-08, illicit=None, illicit_violent=None, self_harm=8.116802746371832e-06, self_harm_instructions=7.912258297437802e-06, self_harm_intent=6.722237594658509e-05, sexual=9.00048416951904e-06, sexual_minors=6.928541552042589e-06, violence=6.768796447431669e-05, violence_graphic=1.6808195368867018e-06, self-harm=8.116802746371832e-06, sexual/minors=6.928541552042589e-06, hate/threatening=8.955092312135093e-08, violence/graphic=1.6808195368867018e-06, self-harm/intent=6.722237594658509e-05, self-harm/instructions=7.912258297437802e-06, harassment/threatening=1.4300552493295982e-06), flagged=False)]\n",
      "Step 1: Input passed moderation check.\n",
      "Error: Invalid JSON string\n",
      "Step 2: Extracted list of products.\n",
      "Step 3: Looked up product information.\n",
      "Step 4: Generated response to user question.\n",
      "Step 6: Model evaluated the response.\n",
      "Step 7: Model approved the response.\n",
      "- The SmartX Pro phone is a high-performance smartphone with a powerful processor, advanced camera features, and a sleek design. It offers a great user experience for both work and entertainment.\n",
      "- The FotoSnap camera is a DSLR camera known for its professional-grade image quality, manual controls, and versatility. It's perfect for photography enthusiasts and professionals looking to capture stunning photos.\n",
      "- Our range of TVs includes various sizes, resolutions, and smart features to suit different preferences. From 4K Ultra HD to OLED displays, we have options for immersive viewing experiences.\n",
      "\n",
      "Do you have any specific questions about the SmartX Pro phone, the FotoSnap camera, or our TVs? Are you looking for any particular features or specifications in these products?\n"
     ]
    }
   ],
   "source": [
    "def process_user_message(user_input, all_messages, debug=True):\n",
    "    delimiter = \"```\"\n",
    "    \n",
    "    # Step 1: Check input to see if it flags the Moderation API or is a prompt injection\n",
    "    response = openai.moderations.create(input=user_input)\n",
    "    moderation_output = response.results\n",
    "    print(moderation_output)\n",
    "    #Robin cHanged\n",
    "    if moderation_output[0].flagged:\n",
    "        print(\"Step 1: Input flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot process this request.\"\n",
    "\n",
    "    if debug: print(\"Step 1: Input passed moderation check.\")\n",
    "    category_and_product_response = utils.find_category_and_product_only(user_input, utils.get_products_and_category())\n",
    "    #print(print(category_and_product_response)\n",
    "    # Step 2: Extract the list of products\n",
    "    category_and_product_list = utils.read_string_to_list(category_and_product_response)\n",
    "    #print(category_and_product_list)\n",
    "\n",
    "    if debug: print(\"Step 2: Extracted list of products.\")\n",
    "\n",
    "    # Step 3: If products are found, look them up\n",
    "    product_information = utils.generate_output_string(category_and_product_list)\n",
    "    if debug: print(\"Step 3: Looked up product information.\")\n",
    "\n",
    "    # Step 4: Answer the user question\n",
    "    system_message = f\"\"\"\n",
    "    You are a customer service assistant for a large electronic store. \n",
    "    Respond in a friendly and helpful tone, with concise answers. \n",
    "    Make sure to ask the user relevant follow-up questions.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"},\n",
    "        {'role': 'assistant', 'content': f\"Relevant product information:\\n{product_information}\"}\n",
    "    ]\n",
    "\n",
    "    final_response = get_completion_from_messages(all_messages + messages)\n",
    "    if debug:print(\"Step 4: Generated response to user question.\")\n",
    "    all_messages = all_messages + messages[1:]\n",
    "\n",
    "    # Step 5: Put the answer through the Moderation API\n",
    "    response = openai.moderations.create(input=final_response)\n",
    "    moderation_output = response.results\n",
    "    # Robin changed\n",
    "    if moderation_output[0].flagged:\n",
    "        if debug: print(\"Step 5: Response flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot provide this information.\"\n",
    "\n",
    "    #if debug: print(\"Step 5: Response passed moderation check.\")\n",
    "\n",
    "    # Step 6: Ask the model if the response answers the initial user query well\n",
    "    user_message = f\"\"\"\n",
    "    Customer message: {delimiter}{user_input}{delimiter}\n",
    "    Agent response: {delimiter}{final_response}{delimiter}\n",
    "\n",
    "    Does the response sufficiently answer the question?\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "    evaluation_response = get_completion_from_messages(messages)\n",
    "    if debug: print(\"Step 6: Model evaluated the response.\")\n",
    "\n",
    "    # Step 7: If yes, use this answer; if not, say that you will connect the user to a human\n",
    "    if \"Y\" in evaluation_response:  # Using \"in\" instead of \"==\" to be safer for model output variation (e.g., \"Y.\" or \"Yes\")\n",
    "        if debug: print(\"Step 7: Model approved the response.\")\n",
    "        return final_response, all_messages\n",
    "    else:\n",
    "        if debug: print(\"Step 7: Model disapproved the response.\")\n",
    "        neg_str = \"I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\"\n",
    "        return neg_str, all_messages\n",
    "\n",
    "user_input = \"tell me about the smartx pro phone and the fotosnap camera, the dslr one. Also what tell me about your tvs\"\n",
    "response,_ = process_user_message(user_input,[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, illicit=None, illicit_violent=None, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False), category_applied_input_types=None, category_scores=CategoryScores(harassment=1.1170866855536588e-05, harassment_threatening=1.4300552493295982e-06, hate=6.180536047395435e-07, hate_threatening=8.955092312135093e-08, illicit=None, illicit_violent=None, self_harm=8.116802746371832e-06, self_harm_instructions=7.912258297437802e-06, self_harm_intent=6.722237594658509e-05, sexual=9.00048416951904e-06, sexual_minors=6.928541552042589e-06, violence=6.768796447431669e-05, violence_graphic=1.6808195368867018e-06, self-harm=8.116802746371832e-06, sexual/minors=6.928541552042589e-06, hate/threatening=8.955092312135093e-08, violence/graphic=1.6808195368867018e-06, self-harm/intent=6.722237594658509e-05, self-harm/instructions=7.912258297437802e-06, harassment/threatening=1.4300552493295982e-06), flagged=False)]\n",
      "Step 1: Input passed moderation check.\n",
      "Error: Invalid JSON string\n",
      "Step 2: Extracted list of products.\n",
      "Step 3: Looked up product information.\n",
      "Step 4: Generated response to user question.\n",
      "Step 6: Model evaluated the response.\n",
      "Step 7: Model approved the response.\n",
      "- The SmartX Pro phone is a high-performance smartphone with a powerful processor, advanced camera features, and a sleek design. It offers a great user experience for both work and entertainment.\n",
      "- The FotoSnap camera is a DSLR camera known for its professional-grade image quality, manual controls, and versatility. It's perfect for photography enthusiasts and professionals looking to capture stunning photos.\n",
      "- Our range of TVs includes various sizes, resolutions, and smart features to suit different preferences. From 4K Ultra HD to OLED displays, we have options for immersive viewing experiences.\n",
      "\n",
      "Do you have any specific questions about the SmartX Pro phone, the FotoSnap camera, or our TVs? Are you looking for any particular features or specifications in these products?\n"
     ]
    }
   ],
   "source": [
    "def process_user_message(user_input, all_messages, debug=True):\n",
    "    delimiter = \"```\"\n",
    "    \n",
    "    # Step 1: Check input to see if it flags the Moderation API or is a prompt injection\n",
    "    response = openai.moderations.create(input=user_input)\n",
    "    moderation_output = response.results\n",
    "    print(moderation_output)\n",
    "    #Robin cHanged\n",
    "    if moderation_output[0].flagged:\n",
    "        print(\"Step 1: Input flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot process this request.\"\n",
    "\n",
    "    if debug: print(\"Step 1: Input passed moderation check.\")\n",
    "    category_and_product_response = utils.find_category_and_product_only(user_input, utils.get_products_and_category())\n",
    "    #print(print(category_and_product_response)\n",
    "    # Step 2: Extract the list of products\n",
    "    category_and_product_list = utils.read_string_to_list(category_and_product_response)\n",
    "    #print(category_and_product_list)\n",
    "\n",
    "    if debug: print(\"Step 2: Extracted list of products.\")\n",
    "\n",
    "    # Step 3: If products are found, look them up\n",
    "    product_information = utils.generate_output_string(category_and_product_list)\n",
    "    if debug: print(\"Step 3: Looked up product information.\")\n",
    "\n",
    "    # Step 4: Answer the user question\n",
    "    system_message = f\"\"\"\n",
    "    You are a customer service assistant for a large electronic store. \n",
    "    Respond in a friendly and helpful tone, with concise answers. \n",
    "    Make sure to ask the user relevant follow-up questions.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"},\n",
    "        {'role': 'assistant', 'content': f\"Relevant product information:\\n{product_information}\"}\n",
    "    ]\n",
    "\n",
    "    final_response = get_completion_from_messages(all_messages + messages)\n",
    "    if debug:print(\"Step 4: Generated response to user question.\")\n",
    "    all_messages = all_messages + messages[1:]\n",
    "\n",
    "    # Step 5: Put the answer through the Moderation API\n",
    "    response = openai.moderations.create(input=final_response)\n",
    "    moderation_output = response.results\n",
    "    # Robin changed\n",
    "    if moderation_output[0].flagged:\n",
    "        if debug: print(\"Step 5: Response flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot provide this information.\"\n",
    "\n",
    "    #if debug: print(\"Step 5: Response passed moderation check.\")\n",
    "\n",
    "    # Step 6: Ask the model if the response answers the initial user query well\n",
    "    user_message = f\"\"\"\n",
    "    Customer message: {delimiter}{user_input}{delimiter}\n",
    "    Agent response: {delimiter}{final_response}{delimiter}\n",
    "\n",
    "    Does the response sufficiently answer the question?\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "    evaluation_response = get_completion_from_messages(messages)\n",
    "    if debug: print(\"Step 6: Model evaluated the response.\")\n",
    "\n",
    "    # Step 7: If yes, use this answer; if not, say that you will connect the user to a human\n",
    "    if \"Y\" in evaluation_response:  # Using \"in\" instead of \"==\" to be safer for model output variation (e.g., \"Y.\" or \"Yes\")\n",
    "        if debug: print(\"Step 7: Model approved the response.\")\n",
    "        return final_response, all_messages\n",
    "    else:\n",
    "        if debug: print(\"Step 7: Model disapproved the response.\")\n",
    "        neg_str = \"I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\"\n",
    "        return neg_str, all_messages\n",
    "\n",
    "user_input = \"tell me about the smartx pro phone and the fotosnap camera, the dslr one. Also what tell me about your tvs\"\n",
    "response,_ = process_user_message(user_input,[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, illicit=None, illicit_violent=None, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False), category_applied_input_types=None, category_scores=CategoryScores(harassment=1.1170866855536588e-05, harassment_threatening=1.4300552493295982e-06, hate=6.180536047395435e-07, hate_threatening=8.955092312135093e-08, illicit=None, illicit_violent=None, self_harm=8.116802746371832e-06, self_harm_instructions=7.912258297437802e-06, self_harm_intent=6.722237594658509e-05, sexual=9.00048416951904e-06, sexual_minors=6.928541552042589e-06, violence=6.768796447431669e-05, violence_graphic=1.6808195368867018e-06, self-harm=8.116802746371832e-06, sexual/minors=6.928541552042589e-06, hate/threatening=8.955092312135093e-08, violence/graphic=1.6808195368867018e-06, self-harm/intent=6.722237594658509e-05, self-harm/instructions=7.912258297437802e-06, harassment/threatening=1.4300552493295982e-06), flagged=False)]\n",
      "Step 1: Input passed moderation check.\n",
      "Error: Invalid JSON string\n",
      "Step 2: Extracted list of products.\n",
      "Step 3: Looked up product information.\n",
      "Step 4: Generated response to user question.\n",
      "Step 6: Model evaluated the response.\n",
      "Step 7: Model approved the response.\n",
      "- The SmartX Pro phone is a high-performance smartphone with a powerful processor, advanced camera features, and a sleek design. It offers a smooth user experience and great connectivity options. Do you have any specific questions about its features or specifications?\n",
      "\n",
      "- The FotoSnap camera is a DSLR camera known for its professional-quality photos and versatile shooting capabilities. It offers manual controls, interchangeable lenses, and high-resolution image sensors. Are you looking for any specific details about its megapixels, lens options, or shooting modes?\n",
      "\n",
      "- Our range of TVs includes various sizes, resolutions, and smart features to suit different preferences. We have 4K Ultra HD TVs, Smart TVs with built-in streaming services, and OLED TVs known for their vibrant colors and deep blacks. Are you interested in a particular size or feature for your TV?\n"
     ]
    }
   ],
   "source": [
    "def process_user_message(user_input, all_messages, debug=True):\n",
    "    delimiter = \"```\"\n",
    "    \n",
    "    # Step 1: Check input to see if it flags the Moderation API or is a prompt injection\n",
    "    response = openai.moderations.create(input=user_input)\n",
    "    moderation_output = response.results\n",
    "    print(moderation_output)\n",
    "    #Robin cHanged\n",
    "    if moderation_output[0].flagged:\n",
    "        print(\"Step 1: Input flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot process this request.\"\n",
    "\n",
    "    if debug: print(\"Step 1: Input passed moderation check.\")\n",
    "    category_and_product_response = utils.find_category_and_product_only(user_input, utils.get_products_and_category())\n",
    "    #print(print(category_and_product_response)\n",
    "    # Step 2: Extract the list of products\n",
    "    category_and_product_list = utils.read_string_to_list(category_and_product_response)\n",
    "    #print(category_and_product_list)\n",
    "\n",
    "    if debug: print(\"Step 2: Extracted list of products.\")\n",
    "\n",
    "    # Step 3: If products are found, look them up\n",
    "    product_information = utils.generate_output_string(category_and_product_list)\n",
    "    if debug: print(\"Step 3: Looked up product information.\")\n",
    "\n",
    "    # Step 4: Answer the user question\n",
    "    system_message = f\"\"\"\n",
    "    You are a customer service assistant for a large electronic store. \n",
    "    Respond in a friendly and helpful tone, with concise answers. \n",
    "    Make sure to ask the user relevant follow-up questions.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"},\n",
    "        {'role': 'assistant', 'content': f\"Relevant product information:\\n{product_information}\"}\n",
    "    ]\n",
    "\n",
    "    final_response = get_completion_from_messages(all_messages + messages)\n",
    "    if debug:print(\"Step 4: Generated response to user question.\")\n",
    "    all_messages = all_messages + messages[1:]\n",
    "\n",
    "    # Step 5: Put the answer through the Moderation API\n",
    "    response = openai.moderations.create(input=final_response)\n",
    "    moderation_output = response.results\n",
    "    # Robin changed\n",
    "    if moderation_output[0].flagged:\n",
    "        if debug: print(\"Step 5: Response flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot provide this information.\"\n",
    "\n",
    "    #if debug: print(\"Step 5: Response passed moderation check.\")\n",
    "\n",
    "    # Step 6: Ask the model if the response answers the initial user query well\n",
    "    user_message = f\"\"\"\n",
    "    Customer message: {delimiter}{user_input}{delimiter}\n",
    "    Agent response: {delimiter}{final_response}{delimiter}\n",
    "\n",
    "    Does the response sufficiently answer the question?\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "    evaluation_response = get_completion_from_messages(messages)\n",
    "    if debug: print(\"Step 6: Model evaluated the response.\")\n",
    "\n",
    "    # Step 7: If yes, use this answer; if not, say that you will connect the user to a human\n",
    "    if \"Y\" in evaluation_response:  # Using \"in\" instead of \"==\" to be safer for model output variation (e.g., \"Y.\" or \"Yes\")\n",
    "        if debug: print(\"Step 7: Model approved the response.\")\n",
    "        return final_response, all_messages\n",
    "    else:\n",
    "        if debug: print(\"Step 7: Model disapproved the response.\")\n",
    "        neg_str = \"I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\"\n",
    "        return neg_str, all_messages\n",
    "\n",
    "user_input = \"tell me about the smartx pro phone and the fotosnap camera, the dslr one. Also what tell me about your tvs\"\n",
    "response,_ = process_user_message(user_input,[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, illicit=None, illicit_violent=None, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False), category_applied_input_types=None, category_scores=CategoryScores(harassment=1.1170866855536588e-05, harassment_threatening=1.4300552493295982e-06, hate=6.180536047395435e-07, hate_threatening=8.955092312135093e-08, illicit=None, illicit_violent=None, self_harm=8.116802746371832e-06, self_harm_instructions=7.912258297437802e-06, self_harm_intent=6.722237594658509e-05, sexual=9.00048416951904e-06, sexual_minors=6.928541552042589e-06, violence=6.768796447431669e-05, violence_graphic=1.6808195368867018e-06, self-harm=8.116802746371832e-06, sexual/minors=6.928541552042589e-06, hate/threatening=8.955092312135093e-08, violence/graphic=1.6808195368867018e-06, self-harm/intent=6.722237594658509e-05, self-harm/instructions=7.912258297437802e-06, harassment/threatening=1.4300552493295982e-06), flagged=False)]\n",
      "Step 1: Input passed moderation check.\n",
      "Error: Invalid JSON string\n",
      "Step 2: Extracted list of products.\n",
      "Step 3: Looked up product information.\n",
      "Step 4: Generated response to user question.\n",
      "Step 6: Model evaluated the response.\n",
      "Step 7: Model approved the response.\n",
      "- The SmartX Pro phone is a high-performance smartphone with a powerful processor, advanced camera features, and a sleek design. It offers a great user experience for both work and entertainment.\n",
      "- The FotoSnap camera is a DSLR camera known for its professional-grade image quality, manual controls, and versatility. It's perfect for photography enthusiasts and professionals looking to capture stunning photos.\n",
      "- Our range of TVs includes various sizes, resolutions, and smart features to suit different preferences. From 4K Ultra HD to OLED displays, we have options for immersive viewing experiences.\n",
      "\n",
      "Do you have any specific questions about the SmartX Pro phone, the FotoSnap camera, or our TVs? Are you looking for any particular features or specifications in these products?\n"
     ]
    }
   ],
   "source": [
    "def process_user_message(user_input, all_messages, debug=True):\n",
    "    delimiter = \"```\"\n",
    "    \n",
    "    # Step 1: Check input to see if it flags the Moderation API or is a prompt injection\n",
    "    response = openai.moderations.create(input=user_input)\n",
    "    moderation_output = response.results\n",
    "    print(moderation_output)\n",
    "    #Robin cHanged\n",
    "    if moderation_output[0].flagged:\n",
    "        print(\"Step 1: Input flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot process this request.\"\n",
    "\n",
    "    if debug: print(\"Step 1: Input passed moderation check.\")\n",
    "    category_and_product_response = utils.find_category_and_product_only(user_input, utils.get_products_and_category())\n",
    "    #print(print(category_and_product_response)\n",
    "    # Step 2: Extract the list of products\n",
    "    category_and_product_list = utils.read_string_to_list(category_and_product_response)\n",
    "    #print(category_and_product_list)\n",
    "\n",
    "    if debug: print(\"Step 2: Extracted list of products.\")\n",
    "\n",
    "    # Step 3: If products are found, look them up\n",
    "    product_information = utils.generate_output_string(category_and_product_list)\n",
    "    if debug: print(\"Step 3: Looked up product information.\")\n",
    "\n",
    "    # Step 4: Answer the user question\n",
    "    system_message = f\"\"\"\n",
    "    You are a customer service assistant for a large electronic store. \n",
    "    Respond in a friendly and helpful tone, with concise answers. \n",
    "    Make sure to ask the user relevant follow-up questions.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"},\n",
    "        {'role': 'assistant', 'content': f\"Relevant product information:\\n{product_information}\"}\n",
    "    ]\n",
    "\n",
    "    final_response = get_completion_from_messages(all_messages + messages)\n",
    "    if debug:print(\"Step 4: Generated response to user question.\")\n",
    "    all_messages = all_messages + messages[1:]\n",
    "\n",
    "    # Step 5: Put the answer through the Moderation API\n",
    "    response = openai.moderations.create(input=final_response)\n",
    "    moderation_output = response.results\n",
    "    # Robin changed\n",
    "    if moderation_output[0].flagged:\n",
    "        if debug: print(\"Step 5: Response flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot provide this information.\"\n",
    "\n",
    "    #if debug: print(\"Step 5: Response passed moderation check.\")\n",
    "\n",
    "    # Step 6: Ask the model if the response answers the initial user query well\n",
    "    user_message = f\"\"\"\n",
    "    Customer message: {delimiter}{user_input}{delimiter}\n",
    "    Agent response: {delimiter}{final_response}{delimiter}\n",
    "\n",
    "    Does the response sufficiently answer the question?\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "    evaluation_response = get_completion_from_messages(messages)\n",
    "    if debug: print(\"Step 6: Model evaluated the response.\")\n",
    "\n",
    "    # Step 7: If yes, use this answer; if not, say that you will connect the user to a human\n",
    "    if \"Y\" in evaluation_response:  # Using \"in\" instead of \"==\" to be safer for model output variation (e.g., \"Y.\" or \"Yes\")\n",
    "        if debug: print(\"Step 7: Model approved the response.\")\n",
    "        return final_response, all_messages\n",
    "    else:\n",
    "        if debug: print(\"Step 7: Model disapproved the response.\")\n",
    "        neg_str = \"I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\"\n",
    "        return neg_str, all_messages\n",
    "\n",
    "user_input = \"tell me about the smartx pro phone and the fotosnap camera, the dslr one. Also what tell me about your tvs\"\n",
    "response,_ = process_user_message(user_input,[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, illicit=None, illicit_violent=None, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False), category_applied_input_types=None, category_scores=CategoryScores(harassment=1.1170866855536588e-05, harassment_threatening=1.4300552493295982e-06, hate=6.180536047395435e-07, hate_threatening=8.955092312135093e-08, illicit=None, illicit_violent=None, self_harm=8.116802746371832e-06, self_harm_instructions=7.912258297437802e-06, self_harm_intent=6.722237594658509e-05, sexual=9.00048416951904e-06, sexual_minors=6.928541552042589e-06, violence=6.768796447431669e-05, violence_graphic=1.6808195368867018e-06, self-harm=8.116802746371832e-06, sexual/minors=6.928541552042589e-06, hate/threatening=8.955092312135093e-08, violence/graphic=1.6808195368867018e-06, self-harm/intent=6.722237594658509e-05, self-harm/instructions=7.912258297437802e-06, harassment/threatening=1.4300552493295982e-06), flagged=False)]\n",
      "Step 1: Input passed moderation check.\n",
      "Error: Invalid JSON string\n",
      "Step 2: Extracted list of products.\n",
      "Step 3: Looked up product information.\n",
      "Step 4: Generated response to user question.\n",
      "Step 6: Model evaluated the response.\n",
      "Step 7: Model approved the response.\n",
      "- **SmartX Pro Phone**: The SmartX Pro Phone is a high-performance smartphone with a powerful processor, advanced camera features, and a sleek design. It offers a large display, long battery life, and fast charging capabilities. Do you have any specific questions about the SmartX Pro Phone?\n",
      "\n",
      "- **FotoSnap Camera (DSLR)**: The FotoSnap Camera is a professional-grade DSLR camera known for its high-quality images, interchangeable lenses, and manual controls. It is ideal for photography enthusiasts and professionals looking to capture stunning photos. Is there anything specific you would like to know about the FotoSnap Camera?\n",
      "\n",
      "- **TVs**: Our store offers a wide range of TVs with various sizes, resolutions, and features such as smart TV capabilities, HDR support, and more. Are you looking for a specific type or size of TV? Let me know your preferences so I can provide you with more detailed information.\n"
     ]
    }
   ],
   "source": [
    "def process_user_message(user_input, all_messages, debug=True):\n",
    "    delimiter = \"```\"\n",
    "    \n",
    "    # Step 1: Check input to see if it flags the Moderation API or is a prompt injection\n",
    "    response = openai.moderations.create(input=user_input)\n",
    "    moderation_output = response.results\n",
    "    print(moderation_output)\n",
    "    #Robin cHanged\n",
    "    if moderation_output[0].flagged:\n",
    "        print(\"Step 1: Input flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot process this request.\"\n",
    "\n",
    "    if debug: print(\"Step 1: Input passed moderation check.\")\n",
    "    category_and_product_response = utils.find_category_and_product_only(user_input, utils.get_products_and_category())\n",
    "    #print(print(category_and_product_response)\n",
    "    # Step 2: Extract the list of products\n",
    "    category_and_product_list = utils.read_string_to_list(category_and_product_response)\n",
    "    #print(category_and_product_list)\n",
    "\n",
    "    if debug: print(\"Step 2: Extracted list of products.\")\n",
    "\n",
    "    # Step 3: If products are found, look them up\n",
    "    product_information = utils.generate_output_string(category_and_product_list)\n",
    "    if debug: print(\"Step 3: Looked up product information.\")\n",
    "\n",
    "    # Step 4: Answer the user question\n",
    "    system_message = f\"\"\"\n",
    "    You are a customer service assistant for a large electronic store. \n",
    "    Respond in a friendly and helpful tone, with concise answers. \n",
    "    Make sure to ask the user relevant follow-up questions.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"},\n",
    "        {'role': 'assistant', 'content': f\"Relevant product information:\\n{product_information}\"}\n",
    "    ]\n",
    "\n",
    "    final_response = get_completion_from_messages(all_messages + messages)\n",
    "    if debug:print(\"Step 4: Generated response to user question.\")\n",
    "    all_messages = all_messages + messages[1:]\n",
    "\n",
    "    # Step 5: Put the answer through the Moderation API\n",
    "    response = openai.moderations.create(input=final_response)\n",
    "    moderation_output = response.results\n",
    "    # Robin changed\n",
    "    if moderation_output[0].flagged:\n",
    "        if debug: print(\"Step 5: Response flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot provide this information.\"\n",
    "\n",
    "    #if debug: print(\"Step 5: Response passed moderation check.\")\n",
    "\n",
    "    # Step 6: Ask the model if the response answers the initial user query well\n",
    "    user_message = f\"\"\"\n",
    "    Customer message: {delimiter}{user_input}{delimiter}\n",
    "    Agent response: {delimiter}{final_response}{delimiter}\n",
    "\n",
    "    Does the response sufficiently answer the question?\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "    evaluation_response = get_completion_from_messages(messages)\n",
    "    if debug: print(\"Step 6: Model evaluated the response.\")\n",
    "\n",
    "    # Step 7: If yes, use this answer; if not, say that you will connect the user to a human\n",
    "    if \"Y\" in evaluation_response:  # Using \"in\" instead of \"==\" to be safer for model output variation (e.g., \"Y.\" or \"Yes\")\n",
    "        if debug: print(\"Step 7: Model approved the response.\")\n",
    "        return final_response, all_messages\n",
    "    else:\n",
    "        if debug: print(\"Step 7: Model disapproved the response.\")\n",
    "        neg_str = \"I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\"\n",
    "        return neg_str, all_messages\n",
    "\n",
    "user_input = \"tell me about the smartx pro phone and the fotosnap camera, the dslr one. Also what tell me about your tvs\"\n",
    "response,_ = process_user_message(user_input,[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, illicit=None, illicit_violent=None, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False), category_applied_input_types=None, category_scores=CategoryScores(harassment=1.1170866855536588e-05, harassment_threatening=1.4300552493295982e-06, hate=6.180536047395435e-07, hate_threatening=8.955092312135093e-08, illicit=None, illicit_violent=None, self_harm=8.116802746371832e-06, self_harm_instructions=7.912258297437802e-06, self_harm_intent=6.722237594658509e-05, sexual=9.00048416951904e-06, sexual_minors=6.928541552042589e-06, violence=6.768796447431669e-05, violence_graphic=1.6808195368867018e-06, self-harm=8.116802746371832e-06, sexual/minors=6.928541552042589e-06, hate/threatening=8.955092312135093e-08, violence/graphic=1.6808195368867018e-06, self-harm/intent=6.722237594658509e-05, self-harm/instructions=7.912258297437802e-06, harassment/threatening=1.4300552493295982e-06), flagged=False)]\n",
      "Step 1: Input passed moderation check.\n",
      "Step 2: Extracted list of products.\n",
      "Step 3: Looked up product information.\n",
      "Step 4: Generated response to user question.\n",
      "Step 6: Model evaluated the response.\n",
      "Step 7: Model approved the response.\n",
      "The SmartX ProPhone is a powerful smartphone with a 6.1-inch display, 128GB storage, 12MP dual camera, and 5G capability. It is priced at $899.99. The FotoSnap DSLR Camera features a 24.2MP sensor, 1080p video recording, a 3-inch LCD screen, and interchangeable lenses, priced at $599.99.\n",
      "\n",
      "As for our TVs, we have a range of options including the CineView 4K TV with a 55-inch display, 4K resolution, HDR, and Smart TV features priced at $599.99. We also offer the CineView 8K TV with a 65-inch display, 8K resolution, HDR, and Smart TV capabilities priced at $2999.99.\n",
      "\n",
      "Do you have any specific questions about the features, pricing, or availability of these products?\n"
     ]
    }
   ],
   "source": [
    "def process_user_message(user_input, all_messages, debug=True):\n",
    "    delimiter = \"```\"\n",
    "    \n",
    "    # Step 1: Check input to see if it flags the Moderation API or is a prompt injection\n",
    "    response = openai.moderations.create(input=user_input)\n",
    "    moderation_output = response.results\n",
    "    print(moderation_output)\n",
    "    #Robin cHanged\n",
    "    if moderation_output[0].flagged:\n",
    "        print(\"Step 1: Input flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot process this request.\"\n",
    "\n",
    "    if debug: print(\"Step 1: Input passed moderation check.\")\n",
    "    category_and_product_response = utils.find_category_and_product_only(user_input, utils.get_products_and_category())\n",
    "    #print(print(category_and_product_response)\n",
    "    # Step 2: Extract the list of products\n",
    "    category_and_product_list = utils.read_string_to_list(category_and_product_response)\n",
    "    #print(category_and_product_list)\n",
    "\n",
    "    if debug: print(\"Step 2: Extracted list of products.\")\n",
    "\n",
    "    # Step 3: If products are found, look them up\n",
    "    product_information = utils.generate_output_string(category_and_product_list)\n",
    "    if debug: print(\"Step 3: Looked up product information.\")\n",
    "\n",
    "    # Step 4: Answer the user question\n",
    "    system_message = f\"\"\"\n",
    "    You are a customer service assistant for a large electronic store. \n",
    "    Respond in a friendly and helpful tone, with concise answers. \n",
    "    Make sure to ask the user relevant follow-up questions.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"},\n",
    "        {'role': 'assistant', 'content': f\"Relevant product information:\\n{product_information}\"}\n",
    "    ]\n",
    "\n",
    "    final_response = get_completion_from_messages(all_messages + messages)\n",
    "    if debug:print(\"Step 4: Generated response to user question.\")\n",
    "    all_messages = all_messages + messages[1:]\n",
    "\n",
    "    # Step 5: Put the answer through the Moderation API\n",
    "    response = openai.moderations.create(input=final_response)\n",
    "    moderation_output = response.results\n",
    "    # Robin changed\n",
    "    if moderation_output[0].flagged:\n",
    "        if debug: print(\"Step 5: Response flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot provide this information.\"\n",
    "\n",
    "    #if debug: print(\"Step 5: Response passed moderation check.\")\n",
    "\n",
    "    # Step 6: Ask the model if the response answers the initial user query well\n",
    "    user_message = f\"\"\"\n",
    "    Customer message: {delimiter}{user_input}{delimiter}\n",
    "    Agent response: {delimiter}{final_response}{delimiter}\n",
    "\n",
    "    Does the response sufficiently answer the question?\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "    evaluation_response = get_completion_from_messages(messages)\n",
    "    if debug: print(\"Step 6: Model evaluated the response.\")\n",
    "\n",
    "    # Step 7: If yes, use this answer; if not, say that you will connect the user to a human\n",
    "    if \"Y\" in evaluation_response:  # Using \"in\" instead of \"==\" to be safer for model output variation (e.g., \"Y.\" or \"Yes\")\n",
    "        if debug: print(\"Step 7: Model approved the response.\")\n",
    "        return final_response, all_messages\n",
    "    else:\n",
    "        if debug: print(\"Step 7: Model disapproved the response.\")\n",
    "        neg_str = \"I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\"\n",
    "        return neg_str, all_messages\n",
    "\n",
    "user_input = \"tell me about the smartx pro phone and the fotosnap camera, the dslr one. Also what tell me about your tvs\"\n",
    "response,_ = process_user_message(user_input,[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, illicit=None, illicit_violent=None, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False), category_applied_input_types=None, category_scores=CategoryScores(harassment=1.1170866855536588e-05, harassment_threatening=1.4300552493295982e-06, hate=6.180536047395435e-07, hate_threatening=8.955092312135093e-08, illicit=None, illicit_violent=None, self_harm=8.116802746371832e-06, self_harm_instructions=7.912258297437802e-06, self_harm_intent=6.722237594658509e-05, sexual=9.00048416951904e-06, sexual_minors=6.928541552042589e-06, violence=6.768796447431669e-05, violence_graphic=1.6808195368867018e-06, self-harm=8.116802746371832e-06, sexual/minors=6.928541552042589e-06, hate/threatening=8.955092312135093e-08, violence/graphic=1.6808195368867018e-06, self-harm/intent=6.722237594658509e-05, self-harm/instructions=7.912258297437802e-06, harassment/threatening=1.4300552493295982e-06), flagged=False)]\n",
      "Step 1: Input passed moderation check.\n",
      "Step 2: Extracted list of products.\n",
      "Step 3: Looked up product information.\n",
      "Step 4: Generated response to user question.\n",
      "Step 6: Model evaluated the response.\n",
      "Step 7: Model approved the response.\n",
      "The SmartX ProPhone is a powerful smartphone with a 6.1-inch display, 128GB storage, 12MP dual camera, and 5G capability. It is priced at $899.99. The FotoSnap DSLR Camera features a 24.2MP sensor, 1080p video recording, 3-inch LCD screen, and interchangeable lenses, priced at $599.99.\n",
      "\n",
      "As for TVs, we have a variety of options:\n",
      "1. CineView 4K TV: 55-inch display, 4K resolution, HDR, Smart TV features, priced at $599.99.\n",
      "2. CineView 8K TV: 65-inch display, 8K resolution, HDR, Smart TV features, priced at $2999.99.\n",
      "3. CineView OLED TV: 55-inch display, 4K resolution, HDR, Smart TV features, priced at $1499.99.\n",
      "\n",
      "Do you have any specific questions about the features, pricing, or availability of these products?\n"
     ]
    }
   ],
   "source": [
    "def process_user_message(user_input, all_messages, debug=True):\n",
    "    delimiter = \"```\"\n",
    "    \n",
    "    # Step 1: Check input to see if it flags the Moderation API or is a prompt injection\n",
    "    response = openai.moderations.create(input=user_input)\n",
    "    moderation_output = response.results\n",
    "    print(moderation_output)\n",
    "    #Robin cHanged\n",
    "    if moderation_output[0].flagged:\n",
    "        print(\"Step 1: Input flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot process this request.\"\n",
    "\n",
    "    if debug: print(\"Step 1: Input passed moderation check.\")\n",
    "    category_and_product_response = utils.find_category_and_product_only(user_input, utils.get_products_and_category())\n",
    "    #print(print(category_and_product_response)\n",
    "    # Step 2: Extract the list of products\n",
    "    category_and_product_list = utils.read_string_to_list(category_and_product_response)\n",
    "    #print(category_and_product_list)\n",
    "\n",
    "    if debug: print(\"Step 2: Extracted list of products.\")\n",
    "\n",
    "    # Step 3: If products are found, look them up\n",
    "    product_information = utils.generate_output_string(category_and_product_list)\n",
    "    if debug: print(\"Step 3: Looked up product information.\")\n",
    "\n",
    "    # Step 4: Answer the user question\n",
    "    system_message = f\"\"\"\n",
    "    You are a customer service assistant for a large electronic store. \n",
    "    Respond in a friendly and helpful tone, with concise answers. \n",
    "    Make sure to ask the user relevant follow-up questions.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"},\n",
    "        {'role': 'assistant', 'content': f\"Relevant product information:\\n{product_information}\"}\n",
    "    ]\n",
    "\n",
    "    final_response = get_completion_from_messages(all_messages + messages)\n",
    "    if debug:print(\"Step 4: Generated response to user question.\")\n",
    "    all_messages = all_messages + messages[1:]\n",
    "\n",
    "    # Step 5: Put the answer through the Moderation API\n",
    "    response = openai.moderations.create(input=final_response)\n",
    "    moderation_output = response.results\n",
    "    # Robin changed\n",
    "    if moderation_output[0].flagged:\n",
    "        if debug: print(\"Step 5: Response flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot provide this information.\"\n",
    "\n",
    "    #if debug: print(\"Step 5: Response passed moderation check.\")\n",
    "\n",
    "    # Step 6: Ask the model if the response answers the initial user query well\n",
    "    user_message = f\"\"\"\n",
    "    Customer message: {delimiter}{user_input}{delimiter}\n",
    "    Agent response: {delimiter}{final_response}{delimiter}\n",
    "\n",
    "    Does the response sufficiently answer the question?\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "    evaluation_response = get_completion_from_messages(messages)\n",
    "    if debug: print(\"Step 6: Model evaluated the response.\")\n",
    "\n",
    "    # Step 7: If yes, use this answer; if not, say that you will connect the user to a human\n",
    "    if \"Y\" in evaluation_response:  # Using \"in\" instead of \"==\" to be safer for model output variation (e.g., \"Y.\" or \"Yes\")\n",
    "        if debug: print(\"Step 7: Model approved the response.\")\n",
    "        return final_response, all_messages\n",
    "    else:\n",
    "        if debug: print(\"Step 7: Model disapproved the response.\")\n",
    "        neg_str = \"I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\"\n",
    "        return neg_str, all_messages\n",
    "\n",
    "user_input = \"tell me about the smartx pro phone and the fotosnap camera, the dslr one. Also what tell me about your tvs\"\n",
    "response,_ = process_user_message(user_input,[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, illicit=None, illicit_violent=None, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False), category_applied_input_types=None, category_scores=CategoryScores(harassment=1.1170866855536588e-05, harassment_threatening=1.4300552493295982e-06, hate=6.180536047395435e-07, hate_threatening=8.955092312135093e-08, illicit=None, illicit_violent=None, self_harm=8.116802746371832e-06, self_harm_instructions=7.912258297437802e-06, self_harm_intent=6.722237594658509e-05, sexual=9.00048416951904e-06, sexual_minors=6.928541552042589e-06, violence=6.768796447431669e-05, violence_graphic=1.6808195368867018e-06, self-harm=8.116802746371832e-06, sexual/minors=6.928541552042589e-06, hate/threatening=8.955092312135093e-08, violence/graphic=1.6808195368867018e-06, self-harm/intent=6.722237594658509e-05, self-harm/instructions=7.912258297437802e-06, harassment/threatening=1.4300552493295982e-06), flagged=False)]\n",
      "Step 1: Input passed moderation check.\n",
      "Step 2: Extracted list of products.\n",
      "Step 3: Looked up product information.\n",
      "Step 4: Generated response to user question.\n",
      "Step 6: Model evaluated the response.\n",
      "Step 7: Model approved the response.\n",
      "The SmartX ProPhone is a powerful smartphone with a 6.1-inch display, 128GB storage, 12MP dual camera, and 5G capability. It is priced at $899.99. The FotoSnap DSLR Camera features a 24.2MP sensor, 1080p video recording, a 3-inch LCD screen, and interchangeable lenses, priced at $599.99.\n",
      "\n",
      "For TVs, we have a range of options including the CineView 4K TV with a 55-inch display, 4K resolution, HDR, and smart TV features priced at $599.99. We also offer the CineView 8K TV with a 65-inch display, 8K resolution, HDR, and smart TV capabilities priced at $2999.99.\n",
      "\n",
      "Do you have any specific questions about the features, warranty, or availability of these products?\n"
     ]
    }
   ],
   "source": [
    "def process_user_message(user_input, all_messages, debug=True):\n",
    "    delimiter = \"```\"\n",
    "    \n",
    "    # Step 1: Check input to see if it flags the Moderation API or is a prompt injection\n",
    "    response = openai.moderations.create(input=user_input)\n",
    "    moderation_output = response.results\n",
    "    print(moderation_output)\n",
    "    #Robin cHanged\n",
    "    if moderation_output[0].flagged:\n",
    "        print(\"Step 1: Input flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot process this request.\"\n",
    "\n",
    "    if debug: print(\"Step 1: Input passed moderation check.\")\n",
    "    category_and_product_response = utils.find_category_and_product_only(user_input, utils.get_products_and_category())\n",
    "    #print(print(category_and_product_response)\n",
    "    # Step 2: Extract the list of products\n",
    "    category_and_product_list = utils.read_string_to_list(category_and_product_response)\n",
    "    #print(category_and_product_list)\n",
    "\n",
    "    if debug: print(\"Step 2: Extracted list of products.\")\n",
    "\n",
    "    # Step 3: If products are found, look them up\n",
    "    product_information = utils.generate_output_string(category_and_product_list)\n",
    "    if debug: print(\"Step 3: Looked up product information.\")\n",
    "\n",
    "    # Step 4: Answer the user question\n",
    "    system_message = f\"\"\"\n",
    "    You are a customer service assistant for a large electronic store. \n",
    "    Respond in a friendly and helpful tone, with concise answers. \n",
    "    Make sure to ask the user relevant follow-up questions.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"},\n",
    "        {'role': 'assistant', 'content': f\"Relevant product information:\\n{product_information}\"}\n",
    "    ]\n",
    "\n",
    "    final_response = get_completion_from_messages(all_messages + messages)\n",
    "    if debug:print(\"Step 4: Generated response to user question.\")\n",
    "    all_messages = all_messages + messages[1:]\n",
    "\n",
    "    # Step 5: Put the answer through the Moderation API\n",
    "    response = openai.moderations.create(input=final_response)\n",
    "    moderation_output = response.results\n",
    "    # Robin changed\n",
    "    if moderation_output[0].flagged:\n",
    "        if debug: print(\"Step 5: Response flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot provide this information.\"\n",
    "\n",
    "    #if debug: print(\"Step 5: Response passed moderation check.\")\n",
    "\n",
    "    # Step 6: Ask the model if the response answers the initial user query well\n",
    "    user_message = f\"\"\"\n",
    "    Customer message: {delimiter}{user_input}{delimiter}\n",
    "    Agent response: {delimiter}{final_response}{delimiter}\n",
    "\n",
    "    Does the response sufficiently answer the question?\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "    evaluation_response = get_completion_from_messages(messages)\n",
    "    if debug: print(\"Step 6: Model evaluated the response.\")\n",
    "\n",
    "    # Step 7: If yes, use this answer; if not, say that you will connect the user to a human\n",
    "    if \"Y\" in evaluation_response:  # Using \"in\" instead of \"==\" to be safer for model output variation (e.g., \"Y.\" or \"Yes\")\n",
    "        if debug: print(\"Step 7: Model approved the response.\")\n",
    "        return final_response, all_messages\n",
    "    else:\n",
    "        if debug: print(\"Step 7: Model disapproved the response.\")\n",
    "        neg_str = \"I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\"\n",
    "        return neg_str, all_messages\n",
    "\n",
    "user_input = \"tell me about the smartx pro phone and the fotosnap camera, the dslr one. Also what tell me about your tvs\"\n",
    "response,_ = process_user_message(user_input,[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Input passed moderation check.\n",
      "Error: Invalid JSON string\n",
      "Step 2: Extracted list of products.\n",
      "Step 3: Looked up product information.\n",
      "Step 4: Generated response to user question.\n",
      "Step 5: Response passed moderation check.\n",
      "Step 6: Model evaluated the response.\n",
      "Step 7: Model approved the response.\n",
      "- The SmartX Pro phone is a high-performance smartphone with a powerful processor, advanced camera features, and a sleek design. It offers a large display and long battery life for all-day use. Do you have any specific questions about its features or specifications?\n",
      "\n",
      "- The FotoSnap camera is a DSLR camera known for its professional-quality photos and versatility. It has interchangeable lenses, manual controls, and advanced shooting modes for creative photography. Are you looking for any specific details about its resolution or features?\n",
      "\n",
      "- Our range of TVs includes various sizes, resolutions, and smart features to suit different preferences. We have 4K Ultra HD TVs, Smart TVs with built-in streaming services, and OLED displays for stunning picture quality. Are you interested in a particular size or feature for your TV?\n"
     ]
    }
   ],
   "source": [
    "def process_user_message(user_input, all_messages, debug=True):\n",
    "    delimiter = \"```\"\n",
    "    \n",
    "    # Step 1: Check input to see if it flags the Moderation API or is a prompt injection\n",
    "    response = openai.moderations.create(input=user_input)\n",
    "    moderation_output = response.results\n",
    "\n",
    "    if moderation_output[0].flagged:\n",
    "        print(\"Step 1: Input flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot process this request.\"\n",
    "\n",
    "    if debug: print(\"Step 1: Input passed moderation check.\")\n",
    "    \n",
    "    category_and_product_response = utils.find_category_and_product_only(user_input, utils.get_products_and_category())\n",
    "    #print(print(category_and_product_response)\n",
    "    # Step 2: Extract the list of products\n",
    "    category_and_product_list = utils.read_string_to_list(category_and_product_response)\n",
    "    #print(category_and_product_list)\n",
    "\n",
    "    if debug: print(\"Step 2: Extracted list of products.\")\n",
    "\n",
    "    # Step 3: If products are found, look them up\n",
    "    product_information = utils.generate_output_string(category_and_product_list)\n",
    "    if debug: print(\"Step 3: Looked up product information.\")\n",
    "\n",
    "    # Step 4: Answer the user question\n",
    "    system_message = f\"\"\"\n",
    "    You are a customer service assistant for a large electronic store. \n",
    "    Respond in a friendly and helpful tone, with concise answers. \n",
    "    Make sure to ask the user relevant follow-up questions.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"},\n",
    "        {'role': 'assistant', 'content': f\"Relevant product information:\\n{product_information}\"}\n",
    "    ]\n",
    "\n",
    "    final_response = get_completion_from_messages(all_messages + messages)\n",
    "    if debug:print(\"Step 4: Generated response to user question.\")\n",
    "    all_messages = all_messages + messages[1:]\n",
    "\n",
    "    # Step 5: Put the answer through the Moderation API\n",
    "    response = openai.moderations.create(input=final_response)\n",
    "    moderation_output = response.results\n",
    "\n",
    "    if moderation_output[0].flagged:\n",
    "        if debug: print(\"Step 5: Response flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot provide this information.\"\n",
    "\n",
    "    if debug: print(\"Step 5: Response passed moderation check.\")\n",
    "\n",
    "    # Step 6: Ask the model if the response answers the initial user query well\n",
    "    user_message = f\"\"\"\n",
    "    Customer message: {delimiter}{user_input}{delimiter}\n",
    "    Agent response: {delimiter}{final_response}{delimiter}\n",
    "\n",
    "    Does the response sufficiently answer the question?\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "    evaluation_response = get_completion_from_messages(messages)\n",
    "    if debug: print(\"Step 6: Model evaluated the response.\")\n",
    "\n",
    "    # Step 7: If yes, use this answer; if not, say that you will connect the user to a human\n",
    "    if \"Y\" in evaluation_response:  # Using \"in\" instead of \"==\" to be safer for model output variation (e.g., \"Y.\" or \"Yes\")\n",
    "        if debug: print(\"Step 7: Model approved the response.\")\n",
    "        return final_response, all_messages\n",
    "    else:\n",
    "        if debug: print(\"Step 7: Model disapproved the response.\")\n",
    "        neg_str = \"I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\"\n",
    "        return neg_str, all_messages\n",
    "\n",
    "user_input = \"tell me about the smartx pro phone and the fotosnap camera, the dslr one. Also what tell me about your tvs\"\n",
    "response,_ = process_user_message(user_input,[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Input passed moderation check.\n",
      "Step 2: Extracted list of products.\n",
      "Step 3: Looked up product information.\n",
      "Step 4: Generated response to user question.\n",
      "Step 5: Response passed moderation check.\n",
      "Step 6: Model evaluated the response.\n",
      "Step 7: Model approved the response.\n",
      "The SmartX ProPhone is a powerful smartphone with a 6.1-inch display, 128GB storage, 12MP dual camera, and 5G capability. It is priced at $899.99. The FotoSnap DSLR Camera features a 24.2MP sensor, 1080p video recording, a 3-inch LCD screen, and interchangeable lenses, priced at $599.99.\n",
      "\n",
      "For TVs, we have a range of options including the CineView 4K TV with a 55-inch display, 4K resolution, HDR, and Smart TV features priced at $599.99. We also offer the CineView 8K TV with a 65-inch display, 8K resolution, HDR, and Smart TV capabilities priced at $2999.99.\n",
      "\n",
      "Do you have any specific questions about the features, warranty, or availability of these products?\n"
     ]
    }
   ],
   "source": [
    "def process_user_message(user_input, all_messages, debug=True):\n",
    "    delimiter = \"```\"\n",
    "    \n",
    "    # Step 1: Check input to see if it flags the Moderation API or is a prompt injection\n",
    "    response = openai.moderations.create(input=user_input)\n",
    "    moderation_output = response.results\n",
    "\n",
    "    if moderation_output[0].flagged:\n",
    "        print(\"Step 1: Input flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot process this request.\"\n",
    "\n",
    "    if debug: print(\"Step 1: Input passed moderation check.\")\n",
    "    \n",
    "    category_and_product_response = utils.find_category_and_product_only(user_input, utils.get_products_and_category())\n",
    "    #print(print(category_and_product_response)\n",
    "    # Step 2: Extract the list of products\n",
    "    category_and_product_list = utils.read_string_to_list(category_and_product_response)\n",
    "    #print(category_and_product_list)\n",
    "\n",
    "    if debug: print(\"Step 2: Extracted list of products.\")\n",
    "\n",
    "    # Step 3: If products are found, look them up\n",
    "    product_information = utils.generate_output_string(category_and_product_list)\n",
    "    if debug: print(\"Step 3: Looked up product information.\")\n",
    "\n",
    "    # Step 4: Answer the user question\n",
    "    system_message = f\"\"\"\n",
    "    You are a customer service assistant for a large electronic store. \n",
    "    Respond in a friendly and helpful tone, with concise answers. \n",
    "    Make sure to ask the user relevant follow-up questions.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"},\n",
    "        {'role': 'assistant', 'content': f\"Relevant product information:\\n{product_information}\"}\n",
    "    ]\n",
    "\n",
    "    final_response = get_completion_from_messages(all_messages + messages)\n",
    "    if debug:print(\"Step 4: Generated response to user question.\")\n",
    "    all_messages = all_messages + messages[1:]\n",
    "\n",
    "    # Step 5: Put the answer through the Moderation API\n",
    "    response = openai.moderations.create(input=final_response)\n",
    "    moderation_output = response.results\n",
    "\n",
    "    if moderation_output[0].flagged:\n",
    "        if debug: print(\"Step 5: Response flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot provide this information.\"\n",
    "\n",
    "    if debug: print(\"Step 5: Response passed moderation check.\")\n",
    "\n",
    "    # Step 6: Ask the model if the response answers the initial user query well\n",
    "    user_message = f\"\"\"\n",
    "    Customer message: {delimiter}{user_input}{delimiter}\n",
    "    Agent response: {delimiter}{final_response}{delimiter}\n",
    "\n",
    "    Does the response sufficiently answer the question?\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "    evaluation_response = get_completion_from_messages(messages)\n",
    "    if debug: print(\"Step 6: Model evaluated the response.\")\n",
    "\n",
    "    # Step 7: If yes, use this answer; if not, say that you will connect the user to a human\n",
    "    if \"Y\" in evaluation_response:  # Using \"in\" instead of \"==\" to be safer for model output variation (e.g., \"Y.\" or \"Yes\")\n",
    "        if debug: print(\"Step 7: Model approved the response.\")\n",
    "        return final_response, all_messages\n",
    "    else:\n",
    "        if debug: print(\"Step 7: Model disapproved the response.\")\n",
    "        neg_str = \"I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\"\n",
    "        return neg_str, all_messages\n",
    "\n",
    "user_input = \"tell me about the smartx pro phone and the fotosnap camera, the dslr one. Also what tell me about your tvs\"\n",
    "response,_ = process_user_message(user_input,[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Input passed moderation check.\n",
      "Error: Invalid JSON string\n",
      "Step 2: Extracted list of products.\n",
      "Step 3: Looked up product information.\n",
      "Step 4: Generated response to user question.\n",
      "Step 5: Response passed moderation check.\n",
      "Step 6: Model evaluated the response.\n",
      "Step 7: Model approved the response.\n",
      "- The SmartX Pro phone is a high-performance smartphone with a powerful processor, advanced camera features, and a sleek design. It offers a great user experience for both work and entertainment.\n",
      "- The FotoSnap camera is a DSLR camera known for its professional-grade image quality, manual controls, and versatility. It's perfect for photography enthusiasts and professionals looking to capture stunning photos.\n",
      "- Our range of TVs includes various sizes, resolutions, and smart features to suit different preferences. From 4K Ultra HD to OLED displays, we have options for immersive viewing experiences.\n",
      "\n",
      "Do you have any specific questions about the SmartX Pro phone, the FotoSnap camera, or our TVs? Are you looking for any particular features or specifications in these products?\n"
     ]
    }
   ],
   "source": [
    "def process_user_message(user_input, all_messages, debug=True):\n",
    "    delimiter = \"```\"\n",
    "    \n",
    "    # Step 1: Check input to see if it flags the Moderation API or is a prompt injection\n",
    "    response = openai.moderations.create(input=user_input)\n",
    "    moderation_output = response.results\n",
    "\n",
    "    if moderation_output[0].flagged:\n",
    "        print(\"Step 1: Input flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot process this request.\"\n",
    "\n",
    "    if debug: print(\"Step 1: Input passed moderation check.\")\n",
    "    \n",
    "    category_and_product_response = utils.find_category_and_product_only(user_input, utils.get_products_and_category())\n",
    "    #print(print(category_and_product_response)\n",
    "    # Step 2: Extract the list of products\n",
    "    category_and_product_list = utils.read_string_to_list(category_and_product_response)\n",
    "    #print(category_and_product_list)\n",
    "\n",
    "    if debug: print(\"Step 2: Extracted list of products.\")\n",
    "\n",
    "    # Step 3: If products are found, look them up\n",
    "    product_information = utils.generate_output_string(category_and_product_list)\n",
    "    if debug: print(\"Step 3: Looked up product information.\")\n",
    "\n",
    "    # Step 4: Answer the user question\n",
    "    system_message = f\"\"\"\n",
    "    You are a customer service assistant for a large electronic store. \n",
    "    Respond in a friendly and helpful tone, with concise answers. \n",
    "    Make sure to ask the user relevant follow-up questions.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"},\n",
    "        {'role': 'assistant', 'content': f\"Relevant product information:\\n{product_information}\"}\n",
    "    ]\n",
    "\n",
    "    final_response = get_completion_from_messages(all_messages + messages)\n",
    "    if debug:print(\"Step 4: Generated response to user question.\")\n",
    "    all_messages = all_messages + messages[1:]\n",
    "\n",
    "    # Step 5: Put the answer through the Moderation API\n",
    "    response = openai.moderations.create(input=final_response)\n",
    "    moderation_output = response.results\n",
    "\n",
    "    if moderation_output[0].flagged:\n",
    "        if debug: print(\"Step 5: Response flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot provide this information.\"\n",
    "\n",
    "    if debug: print(\"Step 5: Response passed moderation check.\")\n",
    "\n",
    "    # Step 6: Ask the model if the response answers the initial user query well\n",
    "    user_message = f\"\"\"\n",
    "    Customer message: {delimiter}{user_input}{delimiter}\n",
    "    Agent response: {delimiter}{final_response}{delimiter}\n",
    "\n",
    "    Does the response sufficiently answer the question?\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "    evaluation_response = get_completion_from_messages(messages)\n",
    "    if debug: print(\"Step 6: Model evaluated the response.\")\n",
    "\n",
    "    # Step 7: If yes, use this answer; if not, say that you will connect the user to a human\n",
    "    if \"Y\" in evaluation_response:  # Using \"in\" instead of \"==\" to be safer for model output variation (e.g., \"Y.\" or \"Yes\")\n",
    "        if debug: print(\"Step 7: Model approved the response.\")\n",
    "        return final_response, all_messages\n",
    "    else:\n",
    "        if debug: print(\"Step 7: Model disapproved the response.\")\n",
    "        neg_str = \"I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\"\n",
    "        return neg_str, all_messages\n",
    "\n",
    "user_input = \"tell me about the smartx pro phone and the fotosnap camera, the dslr one. Also what tell me about your tvs\"\n",
    "response,_ = process_user_message(user_input,[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Input passed moderation check.\n",
      "Step 2: Extracted list of products.\n",
      "Step 3: Looked up product information.\n",
      "Step 4: Generated response to user question.\n",
      "Step 5: Response passed moderation check.\n",
      "Step 6: Model evaluated the response.\n",
      "Step 7: Model approved the response.\n",
      "The SmartX ProPhone is a powerful smartphone with a 6.1-inch display, 128GB storage, 12MP dual camera, and 5G capability. It is priced at $899.99. The FotoSnap DSLR Camera features a 24.2MP sensor, 1080p video recording, a 3-inch LCD screen, and interchangeable lenses, priced at $599.99.\n",
      "\n",
      "For TVs, we have a range of options including the CineView 4K TV with a 55-inch display, 4K resolution, HDR, and Smart TV features priced at $599.99. We also offer the CineView 8K TV with a 65-inch display, 8K resolution, HDR, and Smart TV capabilities priced at $2999.99.\n",
      "\n",
      "Do you have any specific questions about the features, warranty, or availability of these products?\n"
     ]
    }
   ],
   "source": [
    "def process_user_message(user_input, all_messages, debug=True):\n",
    "    delimiter = \"```\"\n",
    "    \n",
    "    # Step 1: Check input to see if it flags the Moderation API or is a prompt injection\n",
    "    response = openai.moderations.create(input=user_input)\n",
    "    moderation_output = response.results\n",
    "\n",
    "    if moderation_output[0].flagged:\n",
    "        print(\"Step 1: Input flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot process this request.\"\n",
    "\n",
    "    if debug: print(\"Step 1: Input passed moderation check.\")\n",
    "    \n",
    "    category_and_product_response = utils.find_category_and_product_only(user_input, utils.get_products_and_category())\n",
    "    #print(print(category_and_product_response)\n",
    "    # Step 2: Extract the list of products\n",
    "    category_and_product_list = utils.read_string_to_list(category_and_product_response)\n",
    "    #print(category_and_product_list)\n",
    "\n",
    "    if debug: print(\"Step 2: Extracted list of products.\")\n",
    "\n",
    "    # Step 3: If products are found, look them up\n",
    "    product_information = utils.generate_output_string(category_and_product_list)\n",
    "    if debug: print(\"Step 3: Looked up product information.\")\n",
    "\n",
    "    # Step 4: Answer the user question\n",
    "    system_message = f\"\"\"\n",
    "    You are a customer service assistant for a large electronic store. \n",
    "    Respond in a friendly and helpful tone, with concise answers. \n",
    "    Make sure to ask the user relevant follow-up questions.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"},\n",
    "        {'role': 'assistant', 'content': f\"Relevant product information:\\n{product_information}\"}\n",
    "    ]\n",
    "\n",
    "    final_response = get_completion_from_messages(all_messages + messages)\n",
    "    if debug:print(\"Step 4: Generated response to user question.\")\n",
    "    all_messages = all_messages + messages[1:]\n",
    "\n",
    "    # Step 5: Put the answer through the Moderation API\n",
    "    response = openai.moderations.create(input=final_response)\n",
    "    moderation_output = response.results\n",
    "\n",
    "    if moderation_output[0].flagged:\n",
    "        if debug: print(\"Step 5: Response flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot provide this information.\"\n",
    "\n",
    "    if debug: print(\"Step 5: Response passed moderation check.\")\n",
    "\n",
    "    # Step 6: Ask the model if the response answers the initial user query well\n",
    "    user_message = f\"\"\"\n",
    "    Customer message: {delimiter}{user_input}{delimiter}\n",
    "    Agent response: {delimiter}{final_response}{delimiter}\n",
    "\n",
    "    Does the response sufficiently answer the question?\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "    evaluation_response = get_completion_from_messages(messages)\n",
    "    if debug: print(\"Step 6: Model evaluated the response.\")\n",
    "\n",
    "    # Step 7: If yes, use this answer; if not, say that you will connect the user to a human\n",
    "    if \"Y\" in evaluation_response:  # Using \"in\" instead of \"==\" to be safer for model output variation (e.g., \"Y.\" or \"Yes\")\n",
    "        if debug: print(\"Step 7: Model approved the response.\")\n",
    "        return final_response, all_messages\n",
    "    else:\n",
    "        if debug: print(\"Step 7: Model disapproved the response.\")\n",
    "        neg_str = \"I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\"\n",
    "        return neg_str, all_messages\n",
    "\n",
    "user_input = \"tell me about the smartx pro phone and the fotosnap camera, the dslr one. Also what tell me about your tvs\"\n",
    "response,_ = process_user_message(user_input,[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Input passed moderation check.\n",
      "Error: Invalid JSON string\n",
      "Step 2: Extracted list of products.\n",
      "Step 3: Looked up product information.\n",
      "Step 4: Generated response to user question.\n",
      "Step 5: Response passed moderation check.\n",
      "Step 6: Model evaluated the response.\n",
      "Step 7: Model approved the response.\n",
      "- The SmartX Pro phone is a high-performance smartphone with a powerful processor, advanced camera features, and a sleek design. It offers a smooth user experience and great connectivity options. Do you have any specific questions about its features or specifications?\n",
      "\n",
      "- The FotoSnap camera is a DSLR camera known for its professional-quality photos and versatile shooting capabilities. It offers manual controls, interchangeable lenses, and high-resolution image sensors for capturing stunning photos. Is there anything specific you would like to know about this camera model?\n",
      "\n",
      "- Our range of TVs includes various sizes, resolutions, and smart features to suit different preferences. We have options like 4K Ultra HD, OLED, QLED, and Smart TVs with built-in streaming services. Are you looking for a particular size or feature in a TV? Let me know how I can assist you further with your TV needs.\n"
     ]
    }
   ],
   "source": [
    "def process_user_message(user_input, all_messages, debug=True):\n",
    "    delimiter = \"```\"\n",
    "    \n",
    "    # Step 1: Check input to see if it flags the Moderation API or is a prompt injection\n",
    "    response = openai.moderations.create(input=user_input)\n",
    "    moderation_output = response.results\n",
    "\n",
    "    if moderation_output[0].flagged:\n",
    "        print(\"Step 1: Input flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot process this request.\"\n",
    "\n",
    "    if debug: print(\"Step 1: Input passed moderation check.\")\n",
    "    \n",
    "    category_and_product_response = utils.find_category_and_product_only(user_input, utils.get_products_and_category())\n",
    "    #print(print(category_and_product_response)\n",
    "    # Step 2: Extract the list of products\n",
    "    category_and_product_list = utils.read_string_to_list(category_and_product_response)\n",
    "    #print(category_and_product_list)\n",
    "\n",
    "    if debug: print(\"Step 2: Extracted list of products.\")\n",
    "\n",
    "    # Step 3: If products are found, look them up\n",
    "    product_information = utils.generate_output_string(category_and_product_list)\n",
    "    if debug: print(\"Step 3: Looked up product information.\")\n",
    "\n",
    "    # Step 4: Answer the user question\n",
    "    system_message = f\"\"\"\n",
    "    You are a customer service assistant for a large electronic store. \n",
    "    Respond in a friendly and helpful tone, with concise answers. \n",
    "    Make sure to ask the user relevant follow-up questions.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"},\n",
    "        {'role': 'assistant', 'content': f\"Relevant product information:\\n{product_information}\"}\n",
    "    ]\n",
    "\n",
    "    final_response = get_completion_from_messages(all_messages + messages)\n",
    "    if debug:print(\"Step 4: Generated response to user question.\")\n",
    "    all_messages = all_messages + messages[1:]\n",
    "\n",
    "    # Step 5: Put the answer through the Moderation API\n",
    "    response = openai.moderations.create(input=final_response)\n",
    "    moderation_output = response.results\n",
    "\n",
    "    if moderation_output[0].flagged:\n",
    "        if debug: print(\"Step 5: Response flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot provide this information.\"\n",
    "\n",
    "    if debug: print(\"Step 5: Response passed moderation check.\")\n",
    "\n",
    "    # Step 6: Ask the model if the response answers the initial user query well\n",
    "    user_message = f\"\"\"\n",
    "    Customer message: {delimiter}{user_input}{delimiter}\n",
    "    Agent response: {delimiter}{final_response}{delimiter}\n",
    "\n",
    "    Does the response sufficiently answer the question?\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "    evaluation_response = get_completion_from_messages(messages)\n",
    "    if debug: print(\"Step 6: Model evaluated the response.\")\n",
    "\n",
    "    # Step 7: If yes, use this answer; if not, say that you will connect the user to a human\n",
    "    if \"Y\" in evaluation_response:  # Using \"in\" instead of \"==\" to be safer for model output variation (e.g., \"Y.\" or \"Yes\")\n",
    "        if debug: print(\"Step 7: Model approved the response.\")\n",
    "        return final_response, all_messages\n",
    "    else:\n",
    "        if debug: print(\"Step 7: Model disapproved the response.\")\n",
    "        neg_str = \"I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\"\n",
    "        return neg_str, all_messages\n",
    "\n",
    "user_input = \"tell me about the smartx pro phone and the fotosnap camera, the dslr one. Also what tell me about your tvs\"\n",
    "response,_ = process_user_message(user_input,[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Input passed moderation check.\n",
      "Error: Invalid JSON string\n",
      "Step 2: Extracted list of products.\n",
      "Step 3: Looked up product information.\n",
      "Step 4: Generated response to user question.\n",
      "Step 5: Response passed moderation check.\n",
      "Step 6: Model evaluated the response.\n",
      "Step 7: Model approved the response.\n",
      "- The SmartX Pro phone is a high-performance smartphone with a powerful processor, advanced camera features, and a sleek design. It offers a great user experience for both work and entertainment.\n",
      "- The FotoSnap camera is a DSLR camera known for its professional-grade image quality, manual controls, and versatility. It's perfect for photography enthusiasts and professionals looking to capture stunning photos.\n",
      "- Our range of TVs includes various sizes, resolutions, and smart features to suit different preferences. From 4K Ultra HD to OLED displays, we have options for immersive viewing experiences.\n",
      "\n",
      "Do you have any specific questions about the SmartX Pro phone, the FotoSnap camera, or our TVs? Are you looking for any particular features or specifications in these products?\n"
     ]
    }
   ],
   "source": [
    "def process_user_message(user_input, all_messages, debug=True):\n",
    "    delimiter = \"```\"\n",
    "    \n",
    "    # Step 1: Check input to see if it flags the Moderation API or is a prompt injection\n",
    "    response = openai.moderations.create(input=user_input)\n",
    "    moderation_output = response.results\n",
    "\n",
    "    if moderation_output[0].flagged:\n",
    "        print(\"Step 1: Input flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot process this request.\"\n",
    "\n",
    "    if debug: print(\"Step 1: Input passed moderation check.\")\n",
    "    \n",
    "    category_and_product_response = utils.find_category_and_product_only(user_input, utils.get_products_and_category())\n",
    "    #print(print(category_and_product_response)\n",
    "    # Step 2: Extract the list of products\n",
    "    category_and_product_list = utils.read_string_to_list(category_and_product_response)\n",
    "    #print(category_and_product_list)\n",
    "\n",
    "    if debug: print(\"Step 2: Extracted list of products.\")\n",
    "\n",
    "    # Step 3: If products are found, look them up\n",
    "    product_information = utils.generate_output_string(category_and_product_list)\n",
    "    if debug: print(\"Step 3: Looked up product information.\")\n",
    "\n",
    "    # Step 4: Answer the user question\n",
    "    system_message = f\"\"\"\n",
    "    You are a customer service assistant for a large electronic store. \n",
    "    Respond in a friendly and helpful tone, with concise answers. \n",
    "    Make sure to ask the user relevant follow-up questions.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"},\n",
    "        {'role': 'assistant', 'content': f\"Relevant product information:\\n{product_information}\"}\n",
    "    ]\n",
    "\n",
    "    final_response = get_completion_from_messages(all_messages + messages)\n",
    "    if debug:print(\"Step 4: Generated response to user question.\")\n",
    "    all_messages = all_messages + messages[1:]\n",
    "\n",
    "    # Step 5: Put the answer through the Moderation API\n",
    "    response = openai.moderations.create(input=final_response)\n",
    "    moderation_output = response.results\n",
    "\n",
    "    if moderation_output[0].flagged:\n",
    "        if debug: print(\"Step 5: Response flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot provide this information.\"\n",
    "\n",
    "    if debug: print(\"Step 5: Response passed moderation check.\")\n",
    "\n",
    "    # Step 6: Ask the model if the response answers the initial user query well\n",
    "    user_message = f\"\"\"\n",
    "    Customer message: {delimiter}{user_input}{delimiter}\n",
    "    Agent response: {delimiter}{final_response}{delimiter}\n",
    "\n",
    "    Does the response sufficiently answer the question?\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "    evaluation_response = get_completion_from_messages(messages)\n",
    "    if debug: print(\"Step 6: Model evaluated the response.\")\n",
    "\n",
    "    # Step 7: If yes, use this answer; if not, say that you will connect the user to a human\n",
    "    if \"Y\" in evaluation_response:  # Using \"in\" instead of \"==\" to be safer for model output variation (e.g., \"Y.\" or \"Yes\")\n",
    "        if debug: print(\"Step 7: Model approved the response.\")\n",
    "        return final_response, all_messages\n",
    "    else:\n",
    "        if debug: print(\"Step 7: Model disapproved the response.\")\n",
    "        neg_str = \"I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\"\n",
    "        return neg_str, all_messages\n",
    "\n",
    "user_input = \"tell me about the smartx pro phone and the fotosnap camera, the dslr one. Also what tell me about your tvs\"\n",
    "response,_ = process_user_message(user_input,[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Input passed moderation check.\n",
      "Error: Invalid JSON string\n",
      "Step 2: Extracted list of products.\n",
      "Step 3: Looked up product information.\n",
      "Step 4: Generated response to user question.\n",
      "Step 5: Response passed moderation check.\n",
      "Step 6: Model evaluated the response.\n",
      "Step 7: Model approved the response.\n",
      "- The SmartX Pro phone is a high-performance smartphone with a powerful processor, advanced camera features, and a sleek design. It offers a great user experience for both work and entertainment.\n",
      "- The FotoSnap camera is a DSLR camera known for its professional-grade image quality, manual controls, and versatility. It's perfect for photography enthusiasts and professionals looking to capture stunning photos.\n",
      "- Our range of TVs includes various sizes, resolutions, and smart features to suit different preferences. From 4K Ultra HD to OLED displays, we have options for immersive viewing experiences.\n",
      "\n",
      "Do you have any specific questions about the SmartX Pro phone, the FotoSnap camera, or our TVs? Are you looking for any particular features or specifications in these products?\n"
     ]
    }
   ],
   "source": [
    "def process_user_message(user_input, all_messages, debug=True):\n",
    "    delimiter = \"```\"\n",
    "    \n",
    "    # Step 1: Check input to see if it flags the Moderation API or is a prompt injection\n",
    "    response = openai.moderations.create(input=user_input)\n",
    "    moderation_output = response.results\n",
    "\n",
    "    if moderation_output[0].flagged:\n",
    "        print(\"Step 1: Input flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot process this request.\"\n",
    "\n",
    "    if debug: print(\"Step 1: Input passed moderation check.\")\n",
    "    \n",
    "    category_and_product_response = utils.find_category_and_product_only(user_input, utils.get_products_and_category())\n",
    "    #print(print(category_and_product_response)\n",
    "    # Step 2: Extract the list of products\n",
    "    category_and_product_list = utils.read_string_to_list(category_and_product_response)\n",
    "    #print(category_and_product_list)\n",
    "\n",
    "    if debug: print(\"Step 2: Extracted list of products.\")\n",
    "\n",
    "    # Step 3: If products are found, look them up\n",
    "    product_information = utils.generate_output_string(category_and_product_list)\n",
    "    if debug: print(\"Step 3: Looked up product information.\")\n",
    "\n",
    "    # Step 4: Answer the user question\n",
    "    system_message = f\"\"\"\n",
    "    You are a customer service assistant for a large electronic store. \n",
    "    Respond in a friendly and helpful tone, with concise answers. \n",
    "    Make sure to ask the user relevant follow-up questions.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"},\n",
    "        {'role': 'assistant', 'content': f\"Relevant product information:\\n{product_information}\"}\n",
    "    ]\n",
    "\n",
    "    final_response = get_completion_from_messages(all_messages + messages)\n",
    "    if debug:print(\"Step 4: Generated response to user question.\")\n",
    "    all_messages = all_messages + messages[1:]\n",
    "\n",
    "    # Step 5: Put the answer through the Moderation API\n",
    "    response = openai.moderations.create(input=final_response)\n",
    "    moderation_output = response.results\n",
    "\n",
    "    if moderation_output[0].flagged:\n",
    "        if debug: print(\"Step 5: Response flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot provide this information.\"\n",
    "\n",
    "    if debug: print(\"Step 5: Response passed moderation check.\")\n",
    "\n",
    "    # Step 6: Ask the model if the response answers the initial user query well\n",
    "    user_message = f\"\"\"\n",
    "    Customer message: {delimiter}{user_input}{delimiter}\n",
    "    Agent response: {delimiter}{final_response}{delimiter}\n",
    "\n",
    "    Does the response sufficiently answer the question?\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "    evaluation_response = get_completion_from_messages(messages)\n",
    "    if debug: print(\"Step 6: Model evaluated the response.\")\n",
    "\n",
    "    # Step 7: If yes, use this answer; if not, say that you will connect the user to a human\n",
    "    if \"Y\" in evaluation_response:  # Using \"in\" instead of \"==\" to be safer for model output variation (e.g., \"Y.\" or \"Yes\")\n",
    "        if debug: print(\"Step 7: Model approved the response.\")\n",
    "        return final_response, all_messages\n",
    "    else:\n",
    "        if debug: print(\"Step 7: Model disapproved the response.\")\n",
    "        neg_str = \"I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\"\n",
    "        return neg_str, all_messages\n",
    "\n",
    "user_input = \"tell me about the smartx pro phone and the fotosnap camera, the dslr one. Also what tell me about your tvs\"\n",
    "response,_ = process_user_message(user_input,[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Input passed moderation check.\n",
      "Error: Invalid JSON string\n",
      "Step 2: Extracted list of products.\n",
      "Step 3: Looked up product information.\n",
      "Step 4: Generated response to user question.\n",
      "Step 5: Response passed moderation check.\n",
      "Step 6: Model evaluated the response.\n",
      "Step 7: Model approved the response.\n",
      "- The SmartX Pro phone is a high-performance smartphone with a powerful processor, advanced camera features, and a sleek design. It offers a great user experience for both work and entertainment.\n",
      "- The FotoSnap camera is a DSLR camera known for its professional-grade image quality, manual controls, and interchangeable lenses. It's perfect for photography enthusiasts and professionals looking to capture stunning photos.\n",
      "- Our range of TVs includes various sizes, resolutions, and smart features to suit different needs. From 4K Ultra HD TVs with vibrant colors and sharp images to smart TVs with built-in streaming services, we have options for every viewing preference.\n",
      "\n",
      "Do you have any specific questions about the SmartX Pro phone, the FotoSnap camera, or our TVs? Are you looking for any particular features or specifications in these products?\n"
     ]
    }
   ],
   "source": [
    "def process_user_message(user_input, all_messages, debug=True):\n",
    "    delimiter = \"```\"\n",
    "    \n",
    "    # Step 1: Check input to see if it flags the Moderation API or is a prompt injection\n",
    "    response = openai.moderations.create(input=user_input)\n",
    "    moderation_output = response.results\n",
    "\n",
    "    if moderation_output[0].flagged:\n",
    "        print(\"Step 1: Input flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot process this request.\"\n",
    "\n",
    "    if debug: print(\"Step 1: Input passed moderation check.\")\n",
    "    \n",
    "    category_and_product_response = utils.find_category_and_product_only(user_input, utils.get_products_and_category())\n",
    "    #print(print(category_and_product_response)\n",
    "    # Step 2: Extract the list of products\n",
    "    category_and_product_list = utils.read_string_to_list(category_and_product_response)\n",
    "    #print(category_and_product_list)\n",
    "\n",
    "    if debug: print(\"Step 2: Extracted list of products.\")\n",
    "\n",
    "    # Step 3: If products are found, look them up\n",
    "    product_information = utils.generate_output_string(category_and_product_list)\n",
    "    if debug: print(\"Step 3: Looked up product information.\")\n",
    "\n",
    "    # Step 4: Answer the user question\n",
    "    system_message = f\"\"\"\n",
    "    You are a customer service assistant for a large electronic store. \n",
    "    Respond in a friendly and helpful tone, with concise answers. \n",
    "    Make sure to ask the user relevant follow-up questions.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"},\n",
    "        {'role': 'assistant', 'content': f\"Relevant product information:\\n{product_information}\"}\n",
    "    ]\n",
    "\n",
    "    final_response = get_completion_from_messages(all_messages + messages)\n",
    "    if debug:print(\"Step 4: Generated response to user question.\")\n",
    "    all_messages = all_messages + messages[1:]\n",
    "\n",
    "    # Step 5: Put the answer through the Moderation API\n",
    "    response = openai.moderations.create(input=final_response)\n",
    "    moderation_output = response.results\n",
    "\n",
    "    if moderation_output[0].flagged:\n",
    "        if debug: print(\"Step 5: Response flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot provide this information.\"\n",
    "\n",
    "    if debug: print(\"Step 5: Response passed moderation check.\")\n",
    "\n",
    "    # Step 6: Ask the model if the response answers the initial user query well\n",
    "    user_message = f\"\"\"\n",
    "    Customer message: {delimiter}{user_input}{delimiter}\n",
    "    Agent response: {delimiter}{final_response}{delimiter}\n",
    "\n",
    "    Does the response sufficiently answer the question?\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "    evaluation_response = get_completion_from_messages(messages)\n",
    "    if debug: print(\"Step 6: Model evaluated the response.\")\n",
    "\n",
    "    # Step 7: If yes, use this answer; if not, say that you will connect the user to a human\n",
    "    if \"Y\" in evaluation_response:  # Using \"in\" instead of \"==\" to be safer for model output variation (e.g., \"Y.\" or \"Yes\")\n",
    "        if debug: print(\"Step 7: Model approved the response.\")\n",
    "        return final_response, all_messages\n",
    "    else:\n",
    "        if debug: print(\"Step 7: Model disapproved the response.\")\n",
    "        neg_str = \"I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\"\n",
    "        return neg_str, all_messages\n",
    "\n",
    "user_input = \"tell me about the smartx pro phone and the fotosnap camera, the dslr one. Also what tell me about your tvs\"\n",
    "response,_ = process_user_message(user_input,[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that collects user and assistant messages over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collect_messages(debug=False):\n",
    "    user_input = inp.value_input\n",
    "    if debug: print(f\"User Input = {user_input}\")\n",
    "    if user_input == \"\":\n",
    "        return\n",
    "    inp.value = ''\n",
    "    global context\n",
    "    #response, context = process_user_message(user_input, context, utils.get_products_and_category(),debug=True)\n",
    "    response, context = process_user_message(user_input, context, debug=False)\n",
    "    context.append({'role':'assistant', 'content':f\"{response}\"})\n",
    "    panels.append(\n",
    "        pn.Row('User:', pn.pane.Markdown(user_input, width=600)))\n",
    "    panels.append(\n",
    "        pn.Row('Assistant:', pn.pane.Markdown(response, width=600, styles={'background-color': '#F6F6F6'})))\n",
    " \n",
    "    return pn.Column(*panels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, illicit=None, illicit_violent=None, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False), category_applied_input_types=None, category_scores=CategoryScores(harassment=1.1170866855536588e-05, harassment_threatening=1.4300552493295982e-06, hate=6.180536047395435e-07, hate_threatening=8.955092312135093e-08, illicit=None, illicit_violent=None, self_harm=8.116802746371832e-06, self_harm_instructions=7.912258297437802e-06, self_harm_intent=6.722237594658509e-05, sexual=9.00048416951904e-06, sexual_minors=6.928541552042589e-06, violence=6.768796447431669e-05, violence_graphic=1.6808195368867018e-06, self-harm=8.116802746371832e-06, sexual/minors=6.928541552042589e-06, hate/threatening=8.955092312135093e-08, violence/graphic=1.6808195368867018e-06, self-harm/intent=6.722237594658509e-05, self-harm/instructions=7.912258297437802e-06, harassment/threatening=1.4300552493295982e-06), flagged=False)]\n",
      "Step 1: Input passed moderation check.\n",
      "Error: Invalid JSON string\n",
      "Step 2: Extracted list of products.\n",
      "Step 3: Looked up product information.\n",
      "Step 4: Generated response to user question.\n",
      "Step 6: Model evaluated the response.\n",
      "Step 7: Model approved the response.\n",
      "- The SmartX Pro phone is a high-performance smartphone with a powerful processor, advanced camera features, and a sleek design. It offers a great user experience for both work and entertainment.\n",
      "- The FotoSnap camera is a DSLR camera known for its professional-quality photos and versatile shooting capabilities. It's a great choice for photography enthusiasts and professionals.\n",
      "- Our range of TVs includes various sizes and features such as 4K resolution, smart TV capabilities, and HDR technology. They offer an immersive viewing experience for movies, sports, and gaming.\n",
      "\n",
      "Do you have any specific questions about the SmartX Pro phone, the FotoSnap camera, or our TVs? Are you looking for any particular features or specifications in these products?\n"
     ]
    }
   ],
   "source": [
    "def process_user_message(user_input, all_messages, debug=True):\n",
    "    delimiter = \"```\"\n",
    "    \n",
    "    # Step 1: Check input to see if it flags the Moderation API or is a prompt injection\n",
    "    response = openai.moderations.create(input=user_input)\n",
    "    moderation_output = response.results\n",
    "    print(moderation_output)\n",
    "    #Robin cHanged\n",
    "    if moderation_output[0].flagged:\n",
    "        print(\"Step 1: Input flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot process this request.\"\n",
    "\n",
    "    if debug: print(\"Step 1: Input passed moderation check.\")\n",
    "    category_and_product_response = utils.find_category_and_product_only(user_input, utils.get_products_and_category())\n",
    "    #print(print(category_and_product_response)\n",
    "    # Step 2: Extract the list of products\n",
    "    category_and_product_list = utils.read_string_to_list(category_and_product_response)\n",
    "    #print(category_and_product_list)\n",
    "\n",
    "    if debug: print(\"Step 2: Extracted list of products.\")\n",
    "\n",
    "    # Step 3: If products are found, look them up\n",
    "    product_information = utils.generate_output_string(category_and_product_list)\n",
    "    if debug: print(\"Step 3: Looked up product information.\")\n",
    "\n",
    "    # Step 4: Answer the user question\n",
    "    system_message = f\"\"\"\n",
    "    You are a customer service assistant for a large electronic store. \n",
    "    Respond in a friendly and helpful tone, with concise answers. \n",
    "    Make sure to ask the user relevant follow-up questions.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"},\n",
    "        {'role': 'assistant', 'content': f\"Relevant product information:\\n{product_information}\"}\n",
    "    ]\n",
    "\n",
    "    final_response = get_completion_from_messages(all_messages + messages)\n",
    "    if debug:print(\"Step 4: Generated response to user question.\")\n",
    "    all_messages = all_messages + messages[1:]\n",
    "\n",
    "    # Step 5: Put the answer through the Moderation API\n",
    "    response = openai.moderations.create(input=final_response)\n",
    "    moderation_output = response.results\n",
    "    # Robin changed\n",
    "    if moderation_output[0].flagged:\n",
    "        if debug: print(\"Step 5: Response flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot provide this information.\"\n",
    "\n",
    "    #if debug: print(\"Step 5: Response passed moderation check.\")\n",
    "\n",
    "    # Step 6: Ask the model if the response answers the initial user query well\n",
    "    user_message = f\"\"\"\n",
    "    Customer message: {delimiter}{user_input}{delimiter}\n",
    "    Agent response: {delimiter}{final_response}{delimiter}\n",
    "\n",
    "    Does the response sufficiently answer the question?\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "    evaluation_response = get_completion_from_messages(messages)\n",
    "    if debug: print(\"Step 6: Model evaluated the response.\")\n",
    "\n",
    "    # Step 7: If yes, use this answer; if not, say that you will connect the user to a human\n",
    "    if \"Y\" in evaluation_response:  # Using \"in\" instead of \"==\" to be safer for model output variation (e.g., \"Y.\" or \"Yes\")\n",
    "        if debug: print(\"Step 7: Model approved the response.\")\n",
    "        return final_response, all_messages\n",
    "    else:\n",
    "        if debug: print(\"Step 7: Model disapproved the response.\")\n",
    "        neg_str = \"I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\"\n",
    "        return neg_str, all_messages\n",
    "\n",
    "user_input = \"tell me about the smartx pro phone and the fotosnap camera, the dslr one. Also what tell me about your tvs\"\n",
    "response,_ = process_user_message(user_input,[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat with the chatbot!\n",
    "Note that the system message includes detailed instructions about what the OrderBot should do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ab7d581d7f44aba43d20563fc81dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BokehModel(combine_events=True, render_bundle={'docs_json': {'3b2f1be2-9752-43ba-af33-d62501289b64': {'version"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input = What are your capabilities\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Markdown.__init__() got an unexpected keyword argument 'style'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/jupyter_bokeh/widgets.py:170\u001b[0m, in \u001b[0;36mBokehModel._sync_model\u001b[0;34m(self, _model, content, _buffers)\u001b[0m\n\u001b[1;32m    168\u001b[0m msg_data \u001b[38;5;241m=\u001b[39m event[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsg_data\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m msg_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbokeh_event\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_document\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrigger_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/bokeh/document/callbacks.py:400\u001b[0m, in \u001b[0;36mDocumentCallbackManager.trigger_event\u001b[0;34m(self, event)\u001b[0m\n\u001b[1;32m    398\u001b[0m         model \u001b[38;5;241m=\u001b[39m model_ref()\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m model:\n\u001b[0;32m--> 400\u001b[0m             \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trigger_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent_callbacks_for_event_name(event\u001b[38;5;241m.\u001b[39mevent_name):\n\u001b[1;32m    403\u001b[0m     cb(event)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/bokeh/util/callback_manager.py:111\u001b[0m, in \u001b[0;36mEventCallbackManager._trigger_event\u001b[0;34m(self, event)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocument \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocument\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotify_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     invoke()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/bokeh/document/callbacks.py:262\u001b[0m, in \u001b[0;36mDocumentCallbackManager.notify_event\u001b[0;34m(self, model, event, callback_invoker)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# TODO (bev): use internal event here to dispatch, rather than hard-coding invocation here\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m \u001b[43minvoke_with_curdoc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_invoker\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/bokeh/document/callbacks.py:453\u001b[0m, in \u001b[0;36minvoke_with_curdoc\u001b[0;34m(doc, f)\u001b[0m\n\u001b[1;32m    450\u001b[0m curdoc: Document\u001b[38;5;241m|\u001b[39mUnlockedDocumentProxy \u001b[38;5;241m=\u001b[39m UnlockedDocumentProxy(doc) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnolock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01melse\u001b[39;00m doc\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m patch_curdoc(curdoc):\n\u001b[0;32m--> 453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/bokeh/util/callback_manager.py:107\u001b[0m, in \u001b[0;36mEventCallbackManager._trigger_event.<locals>.invoke\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m     cast(EventCallbackWithoutEvent, callback)()\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEventCallbackWithEvent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/panel/reactive.py:572\u001b[0m, in \u001b[0;36mSyncable._server_event\u001b[0;34m(self, doc, event)\u001b[0m\n\u001b[1;32m    568\u001b[0m     doc\u001b[38;5;241m.\u001b[39madd_next_tick_callback(\n\u001b[1;32m    569\u001b[0m         partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_coroutine, doc, event) \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    570\u001b[0m     )\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 572\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_comm_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/panel/reactive.py:559\u001b[0m, in \u001b[0;36mSyncable._comm_event\u001b[0;34m(self, doc, event)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_bokeh_event(doc, event)\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 559\u001b[0m     \u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/panel/io/state.py:468\u001b[0m, in \u001b[0;36m_state._handle_exception\u001b[0;34m(self, exception)\u001b[0m\n\u001b[1;32m    466\u001b[0m     config\u001b[38;5;241m.\u001b[39mexception_handler(exception)\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exception, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[0;32m--> 468\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mException of unknown type raised: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, level\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/panel/reactive.py:557\u001b[0m, in \u001b[0;36mSyncable._comm_event\u001b[0;34m(self, doc, event)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 557\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_bokeh_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    559\u001b[0m         state\u001b[38;5;241m.\u001b[39m_handle_exception(e)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/panel/reactive.py:494\u001b[0m, in \u001b[0;36mSyncable._process_bokeh_event\u001b[0;34m(self, doc, event)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_curdoc(doc):\n\u001b[0;32m--> 494\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinished processing bokeh event \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, event)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/panel/widgets/button.py:241\u001b[0m, in \u001b[0;36mButton._process_event\u001b[0;34m(self, event)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_process_event\u001b[39m(\u001b[38;5;28mself\u001b[39m, event: ButtonClick) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrigger\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclicks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/param/parameterized.py:2556\u001b[0m, in \u001b[0;36mParameters.trigger\u001b[0;34m(self_, *param_names)\u001b[0m\n\u001b[1;32m   2554\u001b[0m params \u001b[38;5;241m=\u001b[39m {name: param_values[name] \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m param_names}\n\u001b[1;32m   2555\u001b[0m self_\u001b[38;5;241m.\u001b[39m_TRIGGER \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 2556\u001b[0m \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtriggers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2557\u001b[0m self_\u001b[38;5;241m.\u001b[39m_TRIGGER \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2558\u001b[0m self_\u001b[38;5;241m.\u001b[39m_events \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m events\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/param/parameterized.py:2406\u001b[0m, in \u001b[0;36mParameters.update\u001b[0;34m(self_, arg, **kwargs)\u001b[0m\n\u001b[1;32m   2404\u001b[0m         \u001b[38;5;28;01melif\u001b[39;00m pname \u001b[38;5;129;01min\u001b[39;00m private\u001b[38;5;241m.\u001b[39masync_refs:\n\u001b[1;32m   2405\u001b[0m             refs[pname] \u001b[38;5;241m=\u001b[39m private\u001b[38;5;241m.\u001b[39masync_refs[pname]\n\u001b[0;32m-> 2406\u001b[0m restore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ParametersRestorer(parameters\u001b[38;5;241m=\u001b[39mself_, restore\u001b[38;5;241m=\u001b[39mrestore, refs\u001b[38;5;241m=\u001b[39mrefs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/param/parameterized.py:2439\u001b[0m, in \u001b[0;36mParameters._update\u001b[0;34m(self_, arg, **kwargs)\u001b[0m\n\u001b[1;32m   2437\u001b[0m self_\u001b[38;5;241m.\u001b[39m_BATCH_WATCH \u001b[38;5;241m=\u001b[39m BATCH_WATCH\n\u001b[1;32m   2438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m BATCH_WATCH:\n\u001b[0;32m-> 2439\u001b[0m     \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_call_watchers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2441\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tp \u001b[38;5;129;01min\u001b[39;00m trigger_params:\n\u001b[1;32m   2442\u001b[0m     p \u001b[38;5;241m=\u001b[39m self_[tp]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/param/parameterized.py:2624\u001b[0m, in \u001b[0;36mParameters._batch_call_watchers\u001b[0;34m(self_)\u001b[0m\n\u001b[1;32m   2619\u001b[0m events \u001b[38;5;241m=\u001b[39m [self_\u001b[38;5;241m.\u001b[39m_update_event_type(watcher, event_dict[(name, watcher\u001b[38;5;241m.\u001b[39mwhat)],\n\u001b[1;32m   2620\u001b[0m                                    self_\u001b[38;5;241m.\u001b[39m_TRIGGER)\n\u001b[1;32m   2621\u001b[0m           \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m watcher\u001b[38;5;241m.\u001b[39mparameter_names\n\u001b[1;32m   2622\u001b[0m           \u001b[38;5;28;01mif\u001b[39;00m (name, watcher\u001b[38;5;241m.\u001b[39mwhat) \u001b[38;5;129;01min\u001b[39;00m event_dict]\n\u001b[1;32m   2623\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _batch_call_watchers(self_\u001b[38;5;241m.\u001b[39mself_or_cls, enable\u001b[38;5;241m=\u001b[39mwatcher\u001b[38;5;241m.\u001b[39mqueued, run\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 2624\u001b[0m     \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_watcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwatcher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/param/parameterized.py:2586\u001b[0m, in \u001b[0;36mParameters._execute_watcher\u001b[0;34m(self, watcher, events)\u001b[0m\n\u001b[1;32m   2584\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2585\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2586\u001b[0m         \u001b[43mwatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2587\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Skip:\n\u001b[1;32m   2588\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/panel/param.py:891\u001b[0m, in \u001b[0;36mParamRef._replace_pane\u001b[0;34m(self, force, *args)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 891\u001b[0m         new_object \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobject\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_object \u001b[38;5;129;01mis\u001b[39;00m Skip \u001b[38;5;129;01mand\u001b[39;00m new_object \u001b[38;5;129;01mis\u001b[39;00m Undefined:\n\u001b[1;32m    893\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_layout\u001b[38;5;241m.\u001b[39mloading \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/panel/param.py:1106\u001b[0m, in \u001b[0;36mParamFunction.eval\u001b[0;34m(self, ref)\u001b[0m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval\u001b[39m(\u001b[38;5;28mself\u001b[39m, ref):\n\u001b[0;32m-> 1106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43meval_function_with_deps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/param/parameterized.py:163\u001b[0m, in \u001b[0;36meval_function_with_deps\u001b[0;34m(function)\u001b[0m\n\u001b[1;32m    161\u001b[0m         args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mgetattr\u001b[39m(dep\u001b[38;5;241m.\u001b[39mowner, dep\u001b[38;5;241m.\u001b[39mname) \u001b[38;5;28;01mfor\u001b[39;00m dep \u001b[38;5;129;01min\u001b[39;00m arg_deps)\n\u001b[1;32m    162\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {n: \u001b[38;5;28mgetattr\u001b[39m(dep\u001b[38;5;241m.\u001b[39mowner, dep\u001b[38;5;241m.\u001b[39mname) \u001b[38;5;28;01mfor\u001b[39;00m n, dep \u001b[38;5;129;01min\u001b[39;00m kw_deps\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/param/depends.py:85\u001b[0m, in \u001b[0;36mdepends.<locals>._depends\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_depends\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/param/reactive.py:598\u001b[0m, in \u001b[0;36mbind.<locals>.wrapped\u001b[0;34m(*wargs, **wkwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;129m@depends\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdependencies, watch\u001b[38;5;241m=\u001b[39mwatch)\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39mwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mwkwargs):\n\u001b[1;32m    597\u001b[0m     combined_args, combined_kwargs \u001b[38;5;241m=\u001b[39m combine_arguments(wargs, wkwargs)\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43meval_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcombined_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcombined_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 14\u001b[0m, in \u001b[0;36mcollect_messages\u001b[0;34m(debug)\u001b[0m\n\u001b[1;32m     10\u001b[0m context\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m     11\u001b[0m panels\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     12\u001b[0m     pn\u001b[38;5;241m.\u001b[39mRow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser:\u001b[39m\u001b[38;5;124m'\u001b[39m, pn\u001b[38;5;241m.\u001b[39mpane\u001b[38;5;241m.\u001b[39mMarkdown(user_input, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m600\u001b[39m)))\n\u001b[1;32m     13\u001b[0m panels\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m---> 14\u001b[0m     pn\u001b[38;5;241m.\u001b[39mRow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAssistant:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mpn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpane\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMarkdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m600\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbackground-color\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m#F6F6F6\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pn\u001b[38;5;241m.\u001b[39mColumn(\u001b[38;5;241m*\u001b[39mpanels)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/panel/pane/base.py:310\u001b[0m, in \u001b[0;36mPane.__init__\u001b[0;34m(self, object, **params)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_callbacks\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam\u001b[38;5;241m.\u001b[39mwatch(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_pane, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rerender_params)\n\u001b[1;32m    313\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/panel/pane/base.py:154\u001b[0m, in \u001b[0;36mPaneBase.__init__\u001b[0;34m(self, object, **params)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object_changing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_internal_callbacks\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_callbacks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/panel/reactive.py:635\u001b[0m, in \u001b[0;36mReactive.__init__\u001b[0;34m(self, refs, **params)\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m refs:\n\u001b[1;32m    634\u001b[0m         param\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sync_refs, \u001b[38;5;241m*\u001b[39mrefs, watch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 635\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/panel/reactive.py:125\u001b[0m, in \u001b[0;36mSyncable.__init__\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_themer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;66;03m# Useful when updating model properties which trigger potentially\u001b[39;00m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# recursive events\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_updating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/panel/viewable.py:704\u001b[0m, in \u001b[0;36mViewable.__init__\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    703\u001b[0m     hooks \u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhooks\u001b[39m\u001b[38;5;124m'\u001b[39m, [])\n\u001b[0;32m--> 704\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hooks \u001b[38;5;241m=\u001b[39m hooks\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloading:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/panel/viewable.py:543\u001b[0m, in \u001b[0;36mRenderable.__init__\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_comms \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kernels \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 543\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_found_links \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/panel/viewable.py:302\u001b[0m, in \u001b[0;36mLayoutable.__init__\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdesign\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam\u001b[38;5;241m.\u001b[39mdesign\u001b[38;5;241m.\u001b[39mdefault \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdesign\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mdesign\n\u001b[0;32m--> 302\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/param/parameterized.py:4325\u001b[0m, in \u001b[0;36mParameterized.__init__\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m   4323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mdefault \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m:\n\u001b[1;32m   4324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam\u001b[38;5;241m.\u001b[39m_generate_name()\n\u001b[0;32m-> 4325\u001b[0m refs, deps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_params\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4326\u001b[0m object_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   4328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param__private\u001b[38;5;241m.\u001b[39minitialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/param/parameterized.py:1697\u001b[0m, in \u001b[0;36mas_uninitialized.<locals>.override_initialization\u001b[0;34m(self_, *args, **kw)\u001b[0m\n\u001b[1;32m   1695\u001b[0m original_initialized \u001b[38;5;241m=\u001b[39m parameterized_instance\u001b[38;5;241m.\u001b[39m_param__private\u001b[38;5;241m.\u001b[39minitialized\n\u001b[1;32m   1696\u001b[0m parameterized_instance\u001b[38;5;241m.\u001b[39m_param__private\u001b[38;5;241m.\u001b[39minitialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1697\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1698\u001b[0m parameterized_instance\u001b[38;5;241m.\u001b[39m_param__private\u001b[38;5;241m.\u001b[39minitialized \u001b[38;5;241m=\u001b[39m original_initialized\n\u001b[1;32m   1699\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/param/parameterized.py:1948\u001b[0m, in \u001b[0;36mParameters._setup_params\u001b[0;34m(self_, **params)\u001b[0m\n\u001b[1;32m   1946\u001b[0m desc \u001b[38;5;241m=\u001b[39m self_\u001b[38;5;241m.\u001b[39mcls\u001b[38;5;241m.\u001b[39mget_param_descriptor(name)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# pylint: disable-msg=E1101\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m desc:\n\u001b[0;32m-> 1948\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   1949\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.__init__() got an unexpected \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1950\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeyword argument \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1951\u001b[0m     )\n\u001b[1;32m   1953\u001b[0m pobj \u001b[38;5;241m=\u001b[39m objects\u001b[38;5;241m.\u001b[39mget(name)\n\u001b[1;32m   1954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pobj\u001b[38;5;241m.\u001b[39mallow_refs:\n\u001b[1;32m   1955\u001b[0m     \u001b[38;5;66;03m# Until Parameter.allow_refs=True by default we have to\u001b[39;00m\n\u001b[1;32m   1956\u001b[0m     \u001b[38;5;66;03m# speculatively evaluate a values to check whether they\u001b[39;00m\n\u001b[1;32m   1957\u001b[0m     \u001b[38;5;66;03m# contain a reference and warn the user that the\u001b[39;00m\n\u001b[1;32m   1958\u001b[0m     \u001b[38;5;66;03m# behavior may change in future.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Markdown.__init__() got an unexpected keyword argument 'style'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input = Hi to chatgpt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Markdown.__init__() got an unexpected keyword argument 'style'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/jupyter_bokeh/widgets.py:170\u001b[0m, in \u001b[0;36mBokehModel._sync_model\u001b[0;34m(self, _model, content, _buffers)\u001b[0m\n\u001b[1;32m    168\u001b[0m msg_data \u001b[38;5;241m=\u001b[39m event[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsg_data\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m msg_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbokeh_event\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_document\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrigger_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/bokeh/document/callbacks.py:400\u001b[0m, in \u001b[0;36mDocumentCallbackManager.trigger_event\u001b[0;34m(self, event)\u001b[0m\n\u001b[1;32m    398\u001b[0m         model \u001b[38;5;241m=\u001b[39m model_ref()\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m model:\n\u001b[0;32m--> 400\u001b[0m             \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trigger_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent_callbacks_for_event_name(event\u001b[38;5;241m.\u001b[39mevent_name):\n\u001b[1;32m    403\u001b[0m     cb(event)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/bokeh/util/callback_manager.py:111\u001b[0m, in \u001b[0;36mEventCallbackManager._trigger_event\u001b[0;34m(self, event)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocument \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocument\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotify_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     invoke()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/bokeh/document/callbacks.py:262\u001b[0m, in \u001b[0;36mDocumentCallbackManager.notify_event\u001b[0;34m(self, model, event, callback_invoker)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# TODO (bev): use internal event here to dispatch, rather than hard-coding invocation here\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m \u001b[43minvoke_with_curdoc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_invoker\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/bokeh/document/callbacks.py:453\u001b[0m, in \u001b[0;36minvoke_with_curdoc\u001b[0;34m(doc, f)\u001b[0m\n\u001b[1;32m    450\u001b[0m curdoc: Document\u001b[38;5;241m|\u001b[39mUnlockedDocumentProxy \u001b[38;5;241m=\u001b[39m UnlockedDocumentProxy(doc) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnolock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01melse\u001b[39;00m doc\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m patch_curdoc(curdoc):\n\u001b[0;32m--> 453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/bokeh/util/callback_manager.py:107\u001b[0m, in \u001b[0;36mEventCallbackManager._trigger_event.<locals>.invoke\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m     cast(EventCallbackWithoutEvent, callback)()\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEventCallbackWithEvent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/panel/reactive.py:572\u001b[0m, in \u001b[0;36mSyncable._server_event\u001b[0;34m(self, doc, event)\u001b[0m\n\u001b[1;32m    568\u001b[0m     doc\u001b[38;5;241m.\u001b[39madd_next_tick_callback(\n\u001b[1;32m    569\u001b[0m         partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_coroutine, doc, event) \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    570\u001b[0m     )\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 572\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_comm_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/panel/reactive.py:559\u001b[0m, in \u001b[0;36mSyncable._comm_event\u001b[0;34m(self, doc, event)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_bokeh_event(doc, event)\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 559\u001b[0m     \u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/panel/io/state.py:468\u001b[0m, in \u001b[0;36m_state._handle_exception\u001b[0;34m(self, exception)\u001b[0m\n\u001b[1;32m    466\u001b[0m     config\u001b[38;5;241m.\u001b[39mexception_handler(exception)\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exception, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[0;32m--> 468\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mException of unknown type raised: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, level\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/panel/reactive.py:557\u001b[0m, in \u001b[0;36mSyncable._comm_event\u001b[0;34m(self, doc, event)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 557\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_bokeh_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    559\u001b[0m         state\u001b[38;5;241m.\u001b[39m_handle_exception(e)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/panel/reactive.py:494\u001b[0m, in \u001b[0;36mSyncable._process_bokeh_event\u001b[0;34m(self, doc, event)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_curdoc(doc):\n\u001b[0;32m--> 494\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinished processing bokeh event \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, event)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/panel/widgets/button.py:241\u001b[0m, in \u001b[0;36mButton._process_event\u001b[0;34m(self, event)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_process_event\u001b[39m(\u001b[38;5;28mself\u001b[39m, event: ButtonClick) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrigger\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclicks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/param/parameterized.py:2556\u001b[0m, in \u001b[0;36mParameters.trigger\u001b[0;34m(self_, *param_names)\u001b[0m\n\u001b[1;32m   2554\u001b[0m params \u001b[38;5;241m=\u001b[39m {name: param_values[name] \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m param_names}\n\u001b[1;32m   2555\u001b[0m self_\u001b[38;5;241m.\u001b[39m_TRIGGER \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 2556\u001b[0m \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtriggers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2557\u001b[0m self_\u001b[38;5;241m.\u001b[39m_TRIGGER \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2558\u001b[0m self_\u001b[38;5;241m.\u001b[39m_events \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m events\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/param/parameterized.py:2406\u001b[0m, in \u001b[0;36mParameters.update\u001b[0;34m(self_, arg, **kwargs)\u001b[0m\n\u001b[1;32m   2404\u001b[0m         \u001b[38;5;28;01melif\u001b[39;00m pname \u001b[38;5;129;01min\u001b[39;00m private\u001b[38;5;241m.\u001b[39masync_refs:\n\u001b[1;32m   2405\u001b[0m             refs[pname] \u001b[38;5;241m=\u001b[39m private\u001b[38;5;241m.\u001b[39masync_refs[pname]\n\u001b[0;32m-> 2406\u001b[0m restore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ParametersRestorer(parameters\u001b[38;5;241m=\u001b[39mself_, restore\u001b[38;5;241m=\u001b[39mrestore, refs\u001b[38;5;241m=\u001b[39mrefs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/param/parameterized.py:2439\u001b[0m, in \u001b[0;36mParameters._update\u001b[0;34m(self_, arg, **kwargs)\u001b[0m\n\u001b[1;32m   2437\u001b[0m self_\u001b[38;5;241m.\u001b[39m_BATCH_WATCH \u001b[38;5;241m=\u001b[39m BATCH_WATCH\n\u001b[1;32m   2438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m BATCH_WATCH:\n\u001b[0;32m-> 2439\u001b[0m     \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_call_watchers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2441\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tp \u001b[38;5;129;01min\u001b[39;00m trigger_params:\n\u001b[1;32m   2442\u001b[0m     p \u001b[38;5;241m=\u001b[39m self_[tp]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/param/parameterized.py:2624\u001b[0m, in \u001b[0;36mParameters._batch_call_watchers\u001b[0;34m(self_)\u001b[0m\n\u001b[1;32m   2619\u001b[0m events \u001b[38;5;241m=\u001b[39m [self_\u001b[38;5;241m.\u001b[39m_update_event_type(watcher, event_dict[(name, watcher\u001b[38;5;241m.\u001b[39mwhat)],\n\u001b[1;32m   2620\u001b[0m                                    self_\u001b[38;5;241m.\u001b[39m_TRIGGER)\n\u001b[1;32m   2621\u001b[0m           \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m watcher\u001b[38;5;241m.\u001b[39mparameter_names\n\u001b[1;32m   2622\u001b[0m           \u001b[38;5;28;01mif\u001b[39;00m (name, watcher\u001b[38;5;241m.\u001b[39mwhat) \u001b[38;5;129;01min\u001b[39;00m event_dict]\n\u001b[1;32m   2623\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _batch_call_watchers(self_\u001b[38;5;241m.\u001b[39mself_or_cls, enable\u001b[38;5;241m=\u001b[39mwatcher\u001b[38;5;241m.\u001b[39mqueued, run\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 2624\u001b[0m     \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_watcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwatcher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/param/parameterized.py:2586\u001b[0m, in \u001b[0;36mParameters._execute_watcher\u001b[0;34m(self, watcher, events)\u001b[0m\n\u001b[1;32m   2584\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2585\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2586\u001b[0m         \u001b[43mwatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2587\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Skip:\n\u001b[1;32m   2588\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/panel/param.py:891\u001b[0m, in \u001b[0;36mParamRef._replace_pane\u001b[0;34m(self, force, *args)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 891\u001b[0m         new_object \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobject\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_object \u001b[38;5;129;01mis\u001b[39;00m Skip \u001b[38;5;129;01mand\u001b[39;00m new_object \u001b[38;5;129;01mis\u001b[39;00m Undefined:\n\u001b[1;32m    893\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_layout\u001b[38;5;241m.\u001b[39mloading \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/panel/param.py:1106\u001b[0m, in \u001b[0;36mParamFunction.eval\u001b[0;34m(self, ref)\u001b[0m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval\u001b[39m(\u001b[38;5;28mself\u001b[39m, ref):\n\u001b[0;32m-> 1106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43meval_function_with_deps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/param/parameterized.py:163\u001b[0m, in \u001b[0;36meval_function_with_deps\u001b[0;34m(function)\u001b[0m\n\u001b[1;32m    161\u001b[0m         args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mgetattr\u001b[39m(dep\u001b[38;5;241m.\u001b[39mowner, dep\u001b[38;5;241m.\u001b[39mname) \u001b[38;5;28;01mfor\u001b[39;00m dep \u001b[38;5;129;01min\u001b[39;00m arg_deps)\n\u001b[1;32m    162\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {n: \u001b[38;5;28mgetattr\u001b[39m(dep\u001b[38;5;241m.\u001b[39mowner, dep\u001b[38;5;241m.\u001b[39mname) \u001b[38;5;28;01mfor\u001b[39;00m n, dep \u001b[38;5;129;01min\u001b[39;00m kw_deps\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/param/depends.py:85\u001b[0m, in \u001b[0;36mdepends.<locals>._depends\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_depends\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/param/reactive.py:598\u001b[0m, in \u001b[0;36mbind.<locals>.wrapped\u001b[0;34m(*wargs, **wkwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;129m@depends\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdependencies, watch\u001b[38;5;241m=\u001b[39mwatch)\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39mwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mwkwargs):\n\u001b[1;32m    597\u001b[0m     combined_args, combined_kwargs \u001b[38;5;241m=\u001b[39m combine_arguments(wargs, wkwargs)\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43meval_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcombined_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcombined_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 14\u001b[0m, in \u001b[0;36mcollect_messages\u001b[0;34m(debug)\u001b[0m\n\u001b[1;32m     10\u001b[0m context\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m     11\u001b[0m panels\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     12\u001b[0m     pn\u001b[38;5;241m.\u001b[39mRow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser:\u001b[39m\u001b[38;5;124m'\u001b[39m, pn\u001b[38;5;241m.\u001b[39mpane\u001b[38;5;241m.\u001b[39mMarkdown(user_input, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m600\u001b[39m)))\n\u001b[1;32m     13\u001b[0m panels\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m---> 14\u001b[0m     pn\u001b[38;5;241m.\u001b[39mRow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAssistant:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mpn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpane\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMarkdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m600\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbackground-color\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m#F6F6F6\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pn\u001b[38;5;241m.\u001b[39mColumn(\u001b[38;5;241m*\u001b[39mpanels)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/panel/pane/base.py:310\u001b[0m, in \u001b[0;36mPane.__init__\u001b[0;34m(self, object, **params)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_callbacks\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam\u001b[38;5;241m.\u001b[39mwatch(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_pane, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rerender_params)\n\u001b[1;32m    313\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/panel/pane/base.py:154\u001b[0m, in \u001b[0;36mPaneBase.__init__\u001b[0;34m(self, object, **params)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object_changing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_internal_callbacks\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_callbacks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/panel/reactive.py:635\u001b[0m, in \u001b[0;36mReactive.__init__\u001b[0;34m(self, refs, **params)\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m refs:\n\u001b[1;32m    634\u001b[0m         param\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sync_refs, \u001b[38;5;241m*\u001b[39mrefs, watch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 635\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/panel/reactive.py:125\u001b[0m, in \u001b[0;36mSyncable.__init__\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_themer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;66;03m# Useful when updating model properties which trigger potentially\u001b[39;00m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# recursive events\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_updating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/panel/viewable.py:704\u001b[0m, in \u001b[0;36mViewable.__init__\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    703\u001b[0m     hooks \u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhooks\u001b[39m\u001b[38;5;124m'\u001b[39m, [])\n\u001b[0;32m--> 704\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hooks \u001b[38;5;241m=\u001b[39m hooks\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloading:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/panel/viewable.py:543\u001b[0m, in \u001b[0;36mRenderable.__init__\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_comms \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kernels \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 543\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_found_links \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/panel/viewable.py:302\u001b[0m, in \u001b[0;36mLayoutable.__init__\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdesign\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam\u001b[38;5;241m.\u001b[39mdesign\u001b[38;5;241m.\u001b[39mdefault \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdesign\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mdesign\n\u001b[0;32m--> 302\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/param/parameterized.py:4325\u001b[0m, in \u001b[0;36mParameterized.__init__\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m   4323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mdefault \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m:\n\u001b[1;32m   4324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam\u001b[38;5;241m.\u001b[39m_generate_name()\n\u001b[0;32m-> 4325\u001b[0m refs, deps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_params\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4326\u001b[0m object_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   4328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param__private\u001b[38;5;241m.\u001b[39minitialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/param/parameterized.py:1697\u001b[0m, in \u001b[0;36mas_uninitialized.<locals>.override_initialization\u001b[0;34m(self_, *args, **kw)\u001b[0m\n\u001b[1;32m   1695\u001b[0m original_initialized \u001b[38;5;241m=\u001b[39m parameterized_instance\u001b[38;5;241m.\u001b[39m_param__private\u001b[38;5;241m.\u001b[39minitialized\n\u001b[1;32m   1696\u001b[0m parameterized_instance\u001b[38;5;241m.\u001b[39m_param__private\u001b[38;5;241m.\u001b[39minitialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1697\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1698\u001b[0m parameterized_instance\u001b[38;5;241m.\u001b[39m_param__private\u001b[38;5;241m.\u001b[39minitialized \u001b[38;5;241m=\u001b[39m original_initialized\n\u001b[1;32m   1699\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/param/parameterized.py:1948\u001b[0m, in \u001b[0;36mParameters._setup_params\u001b[0;34m(self_, **params)\u001b[0m\n\u001b[1;32m   1946\u001b[0m desc \u001b[38;5;241m=\u001b[39m self_\u001b[38;5;241m.\u001b[39mcls\u001b[38;5;241m.\u001b[39mget_param_descriptor(name)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# pylint: disable-msg=E1101\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m desc:\n\u001b[0;32m-> 1948\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   1949\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.__init__() got an unexpected \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1950\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeyword argument \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1951\u001b[0m     )\n\u001b[1;32m   1953\u001b[0m pobj \u001b[38;5;241m=\u001b[39m objects\u001b[38;5;241m.\u001b[39mget(name)\n\u001b[1;32m   1954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pobj\u001b[38;5;241m.\u001b[39mallow_refs:\n\u001b[1;32m   1955\u001b[0m     \u001b[38;5;66;03m# Until Parameter.allow_refs=True by default we have to\u001b[39;00m\n\u001b[1;32m   1956\u001b[0m     \u001b[38;5;66;03m# speculatively evaluate a values to check whether they\u001b[39;00m\n\u001b[1;32m   1957\u001b[0m     \u001b[38;5;66;03m# contain a reference and warn the user that the\u001b[39;00m\n\u001b[1;32m   1958\u001b[0m     \u001b[38;5;66;03m# behavior may change in future.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Markdown.__init__() got an unexpected keyword argument 'style'"
     ]
    }
   ],
   "source": [
    "panels = [] # collect display \n",
    "\n",
    "context = [ {'role':'system', 'content':\"You are Service Assistant\"} ]  \n",
    "\n",
    "inp = pn.widgets.TextInput( placeholder='Enter text here')\n",
    "button_conversation = pn.widgets.Button(name=\"Service Assistant\")\n",
    "\n",
    "interactive_conversation = pn.bind(collect_messages, button_conversation)\n",
    "\n",
    "dashboard = pn.Column(\n",
    "    inp,\n",
    "    pn.Row(button_conversation),\n",
    "    pn.panel(interactive_conversation, loading_indicator=True, height=300),\n",
    ")\n",
    "\n",
    "dashboard"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
